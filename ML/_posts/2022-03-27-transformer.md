### 0. ML 모델 이해를 위한 팁

\- 다음 순서를 따르는 것이 복잡한 ML 모델을 이해하는 데 도움이 된다.

(1) 모델이 풀고자 하는 문제를 파악한다. (분류, 생성, ...)

(2) 훈련 단계가 아니라 추론 단계부터 먼저 이해한다. 즉, '이미 학습이 이루어진' 모델이 어떤 입력을 받아 어떤 출력을 내는지를 먼저 이해한다.

(3) 훈련 단계에서는 모델의 어떤 파라미터가 학습이 이루어지는지(구조가 복잡한 모델에서는 입력으로 주어지는 데이터와 파라미터가 헷갈리기 쉽다), 그리고 그 파라미터가 어떤 방식으로 학습이 이루어지는지(=어떤 에러함수를 사용하는지)를 이해한다. 


### 1. transformer 이전 모델들

\- RNN: 시간에 따라 변화하는, 길이가 있는 데이터를 처리하는 인공신경망으로서 가장 처음 제시된 것. 데이터의 길이가 길어지면 앞 데이터와 뒤 데이터 사이의 의존성을 모델링하기 어렵고, 연산에 오랜 시간이 걸린다는 단점이 있다.

\- TextCNN: 기존 CNN과 같은 방식으로 신경망을 구성하되 입력 데이터로 텍스트를 받는 CNN. RNN보다는 훨씬 빠른 성능을 보인다. 그러나 긴 데이터의 앞뒤 사이 의존성을 처리하는 성능은 훨씬 나쁘다. 다만 텍스트 분류 문제에서는 나쁘지 않는 성능을 보이는 경향이 있다. 


### 2. transformer의 개요

\- 2017년 구글에서 'Attention is all you need'라는 제목의 논문으로 처음 소개한 모델. 자연어처리를 하는 인공신경망 모델으로서는 전과 다른 혁신적인 수준의 성능을 보여 NLP의 역사에서 매우 중요한 모델로 여겨진다. 현재 널리 쓰이는 BERT, GPT가 이 transformer에서 파생된 모델이다.

\- RNN, CNN과는 전혀 다른 구조를 가지며, 오직 attention이라는 구조만으로 텍스트를 처리한다. 성능이 뛰어나다는 것 외에도, RNN과는 달리 병렬화가 용이하다는 장점이 있다.

\- transformer는 순차적 입력에 대하여 순차적 출력을 반환하는 모델이다.


### 3. transformer의 구조

#### 1) 개요

\- 크게 encoder와 decoder 두 부분으로 나뉜다. 입력 데이터가 encoder를 통과하여 만들어진 정보가 decoder에 전달되며, decoder는 이 정보를 통해 출력 데이터를 만들어 반환한다.

- encoder: 똑같은 구조가 여러 겹의 층으로 쌓여 있으며(단 각 층의 파라미터가 갖는 값은 서로 다르다), 입력 데이터가 가장 앞의 층으로 입력되어 그 결과가 바로 다음 층의 입력으로 전달되는 과정이 순차적으로 반복된다.

- decoder: encoder와 마찬가지로 똑같은 구조가 여러 겹의 층으로 쌓여 있다. 또 encoder에서 만들어진 출력이 decoder의 가장 앞의 층으로 입력이 된다는 점까지도 같으나, 그 다음층으로 넘어갈 때 바로 앞의 층에서 만들어진 입력과 함께 encoder에서 만들어졌던 그 출력이 또 바로 다음 층의 입력으로 주어진다는 점이 차이가 있다.

#### 2) encoder의 구조

\- 크게 self-attention 모듈 부분과 feed forward 모듈 부분으로 나뉘며, 각각 모두 내부에 신경망을 갖고 있다. 여러 단어로 이루어진 문장이 encoder의 입력으로 주어진다 할 때, encoder를 거친 결과 문장은 입력으로 주어진 문장과 같은 단어 개수를 갖고 있다.

- self-attention: encoder의 입력으로 들어오는 단어 벡터는 먼저 self-attention 모듈의 입력이 된다. self-attention은 단어의 의미를 이해할 때 그 단어만 사용하는 게 아니라 그 주변 문맥을 사용한다. 입력 단어 벡터는 self-attention을 거쳐 그 입력 단어 벡터 주변 단어 벡터들의 문맥을 모두 반영한 새 단어 벡터가 된다.

- feed forward: self-attention의 출력이 곧바로 feed forward의 입력이 된다. self-attention과 달리, 입력으로 들어오는 단어 벡터들이 서로 주변에 있다 하여 이를 서로 값 계산에 반영하지는 않고 독립적으로 입력 단어 벡터를 처리하여 출력 벡터를 반환한다. (따라서 완전한 병렬화가 가능하다.)