### 0. ML 모델 이해를 위한 팁

\- 다음 순서를 따르는 것이 복잡한 ML 모델을 이해하는 데 도움이 된다.

(1) 모델이 풀고자 하는 문제를 파악한다. (분류, 생성, ...)

(2) 훈련 단계가 아니라 추론 단계부터 먼저 이해한다. 즉, '이미 학습이 이루어진' 모델이 어떤 입력을 받아 어떤 출력을 내는지를 먼저 이해한다.

(3) 훈련 단계에서는 모델의 어떤 파라미터가 학습이 이루어지는지(구조가 복잡한 모델에서는 입력으로 주어지는 데이터와 파라미터가 헷갈리기 쉽다), 그리고 그 파라미터가 어떤 방식으로 학습이 이루어지는지(=어떤 에러함수를 사용하는지)를 이해한다. 


### 1. transformer 이전 모델들

\- RNN: 시간에 따라 변화하는, 길이가 있는 데이터를 처리하는 인공신경망으로서 가장 처음 제시된 것. 데이터의 길이가 길어지면 앞 데이터와 뒤 데이터 사이의 의존성을 모델링하기 어렵고, 연산에 오랜 시간이 걸린다는 단점이 있다.

\- TextCNN: 기존 CNN과 같은 방식으로 신경망을 구성하되 입력 데이터로 텍스트를 받는 CNN. RNN보다는 훨씬 빠른 성능을 보인다. 그러나 긴 데이터의 앞뒤 사이 의존성을 처리하는 성능은 훨씬 나쁘다. 다만 텍스트 분류 문제에서는 나쁘지 않는 성능을 보이는 경향이 있다. 


### 2. transformer의 개요

\- 'Attention is all you need'라는 제목의 논문으로 처음 소개된 모델. 자연어처리를 하는 인공신경망 모델으로서는 전과 다른 혁신적인 수준의 성능을 보여 NLP의 역사에서 매우 중요한 모델로 여겨진다. 현재 널리 쓰이는 BERT, GPT가 이 transformer에서 파생된 모델이다.

\- RNN, CNN과는 전혀 다른 구조를 가지며, 오직 attention이라는 구조만으로 텍스트를 처리한다. 성능이 뛰어나다는 것 외에도, RNN과는 달리 병렬화가 용이하다는 장점이 있다.

\- transformer는 순차적 입력에 대하여 순차적 출력을 반환하는 모델(seq2seq)이다.


### 3. transformer의 구조

\- 크게 encoder와 decoder 두 부분으로 나뉜다. 입력 데이터가 encoder를 통과하여 만들어진 정보가 decoder에 전달되며, decoder는 이 정보를 통해 출력 데이터를 만들어 반환한다.