### 1. 개념

- synonym

- similarity

- relatedness(연관성)

  - sementic field: 특정 주제를 공유하는 단어들. 예를 들면, 의사, 간호사, 진료 같은 단어들은 '병원'이라는 하나의 주제를 공유한다.

  - semantic frame: 어떤 행위를 하는 주체의 역할에 관한 단어들. 예를 들면, 장을 보는 사람이 있을 때 이 사람은 물건을 담고, 결제를 하고, 거스름돈을 받는 등의 행위를 한다. 여기서 '물건 담기', '결제하기', '거스름돈 받기'는 하나의 sementic frame을 공유한다.



### 2. 단어를 벡터로 표현하기

\- 어떤 두 단어의 주변 단어 분포가 거의 같다면 그 두 단어는 유사어이다. 예를 들어 ong choi라는 단어가 delicious with garlic, over rice, leaves 같은 표현들과 함께 쓰이고 있다면, ong choi가 '마늘 양념이 되고 밥 위에 얹어 먹는 채소'일 거라 짐작할 수 있으며 비슷한 표현들과 쓰인 다른 단어들이 ong choi와 유사한 단어 관계라고 짐작할 수 있다. 이처럼 단어들은 주변의 환경(주변의 단어들의 분포)에 의해 의미가 결정된다고 할 수 있다.

\- 단어의 의미를 분포적 유사성(distributional similarity)을 통해 벡터로 표현할 수 있다. 이렇게 표현된 벡터를 embedding이라 한다. embedding을 사용하면, 테스트 데이터가 훈련 데이터에 없는 단어를 사용한다 하더라도 좋은 예측 결과를 낸다는 등의 이점이 있다.

\- 현재 쓰이는 대부분의 언어모델은 dense vector를 사용하나, 검색엔진의 경우에는 sparse vector를 사용한다.

- sparse vector: 벡터 하나의 차원이 수만 차원이 되며, 그러면서 각 벡터의 대부분의 성분이 0이라 메모리 낭비가 크다는 단점이 있다.

- dense vector: 벡터 하나의 차원이 수천 이하여서 ML 학습에 사용할 때 적은 파라미터만 사용하고, 또 유사한 벡터끼리는 각 성분값의 차이가 크지 않는 값을 가져 동의어, 유사어를 잘 표현한다는 장점이 있다.



### 3. tf-idf

\- 문서 하나를 그 문서를 구성하는 각 단어의 빈도를 성분으로 하는 벡터로 표현한 것을 document vector라 한다. 이를 통해 각 문서의 유사도를 파악할 수 있다.

- 한편, 한 문서에서 어떤 단어 하나가 사용된 빈도를 tf(term frequency)라 한다.

\- 여러 문서가 있을 때, 어떤 단어를 그 단어가 각 문서에서 사용된 빈도를 성분으로 하는 벡터로 표현한 것을 term vector라 한다. 이를 통해 각 단어의 의미와 단어간 유사도를 파악할 수 있다. 

- 한편, 어떤 단어가 사용된 문서의 개수를 df(document frequency)라 하며, 이의 역수를 idf(inverse document frequency)라 한다.

\- a, the, of 같이 어떤 문서에건 흔히 쓰이는 단어는 그 단어가 문서에서 사용된 빈도(tf)만을 기준으로 벡터로 표현하는 경우에는 그 벡터만으로 그 의미를 파악할 수 없다. 따라서 tf 대신에 단어가 '어떤 문서에건 흔히 쓰이는 단어인지 아닌지를 나타내는 지표'를 찾아 이를 반영한 값을 사용해야 한다. tf에 idf를 곱한 값을 사용하는 방식이 널리 쓰이며 이를 tf-idf라 한다.


### 4. Word2vec

#### 1) 개요

\- '어떤 단어가 주어질 때, 그 단어 주변에 특정 단어가 나타날 확률을 구한다'라는 관점에서 접근하여 단어를 벡터로 표현하는 방법이다. 

\- 인위적으로 레이블을 제공하는 지도학습 방식을 사용하지 않고 단어를 벡터로 표현할 수 있다.

#### 2) skip-gram

\- \\(w_t\\)라는 단어가 주어질 때, 그 앞뒤 각각 \\(m\\)개씩(이를 window라 한다) 총 \\(2m\\)개의 단어가 \\(w_{t-m}, \cdots, w_{t+m}\\)으로 분포할 확률 \\(\prod^m_{j=1} p(w_{t-j} | w_t)\\)를 최대로 하는 \\(w_{t-m}, \cdots, w_{t+m}\\)의 조합을 구하는 문제를 생각할 수 있다.

\- 문제를 확장하여, 입력 단어 \\(w\\)에 대하여 출력 단어로 \\(c\\)가 나올 확률 \\(p(c|w)\\)에 대해 생각해 보자. \\(w, c\\)를 각각 one-hot vector로 표현한 벡터를 \\(x_w, x_c\\)라 하고 \\(W, C\\)를 각각 여러 개의 입력, 출력 단어들을 벡터로 표현한 것을 모아놓은 행렬이라 할 때, \\(v_w = (x_w^T W)^T\\), \\(u_w = (x_c^T C)^T\\)라는 값을 정의할 수 있다. 그렇다면 \\(p(c|w) = { exp(u_c^T v_w) \over {\sum_{c'} exp(u_{c'}^T v_w)}\\) 로 쓸 수 있다. 

\- 이 모델에 따라 확률값을 계산하는 경우 연산량이 지나치게 많아진다는 단점이 있다. 이 식의 분모를 신경망의 파라미터로 보고 신경망을 학습하는 방식으로 학습할 경우 이를 완화할 수 있다(이를 noise-constrastive estimation이라 한다). (이를 단순화 한 것이 negative sampling이다.)