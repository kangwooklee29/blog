### 1. 여러 개념들

- vocabulary: 단어의 집합.


- corpus: 대용량 문서들의 집합. 어떤 언어로 돼있는지, 어떤 분야의 문서들인지, 작성자가 어떤 배경이 있는지 등을 기준으로 각 corpus의 성격이 결정된다. 


- token: 한 vocabulary를 구성하는 말의 단위. 어떤 corpus를 NLP 알고리즘의 훈련 데이터로 하기 위해서는 우선 그 corpus를 token 단위로 나누는 전처리가 필요하다(이를 tokenizing이라 한다). token의 단위로 단어를 쓰는 경우도 많지만, 단어보다 더 작은 단위(subword)를 token으로 할 필요성이 커 subword 단위로 tokenizing을 하는 경우가 더 많다.

- type: 한 vocabulary 안에 등장하는 token들의 종류. 수억 개의 token으로 이루어진 corpus라 하더라도 그 corpus를 구성하는 type의 개수는 수만개 이하일 수 있다.



### 2. subword tokenizing 알고리즘들

\- corpus에서 token으로 이루어진 vocabulary를 추출해내는 것을 token learning이라 하며, vocabulary에 없는 새 단어를 그 vocabulary의 token들로 쪼개는 것을 token segmentation이라 한다.


#### 1) BPE(byte pair encoding)


\- 다음 방식으로 token learning을 수행한다.

(1) 주어진 corpus를 길이가 1인 문자들로 쪼개 vocabulary를 채우고, corpus를 그 token들로 tokenize 해둔다.

(2) vocabulary에 있는 token 중 corpus에 연속으로 등장하는 두 token 조합들의 빈도를 세, 가장 빈도가 높은 조합을 하나로 이어 새 token을 만들어 vocabulary에 추가한다.

(3) tokenize 된 corpus에서, 새로 추가된 token을 구성하던 token이 있던 위치의 token을 새로 추가된 token으로 대체한다. (예: l o w e r -> l o w er)

(4) 위 2-3단계를 충분히 많은 횟수만큼 반복한다. (이 횟수는 이 알고리즘을 수행하는 경우마다 그 상황에 맞는 적절한 수로 달라질 수 있다.)

\- 다음 방식으로 token segmentation을 수행한다.

(1) 주어진 새 단어를 길이가 1인 문자들로 쪼개 tokenize 해둔다.

(2) 새 단어에서 vocabulary에 있는 token과 일치하는 부분을 찾는다. 이때, vocabulary에서 가장 빈도수가 높은 token부터 차례대로 찾는다. 만약 일부분과 일치하는 token을 발견했다면 그 부분을 그 token으로 대체한다. (예: n e w e r -> n e w er)

(3) vocabulary의 모든 token과 비교할 때까지 위 2단계를 반복한다.