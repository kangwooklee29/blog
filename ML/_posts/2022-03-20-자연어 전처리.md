### 1. 여러 개념들

- vocabulary: 단어의 집합.


- corpus: 대용량 문서들의 집합. 어떤 언어로 돼있는지, 어떤 분야의 문서들인지, 작성자가 어떤 배경이 있는지 등을 기준으로 각 corpus의 성격이 결정된다. 


- token: 한 vocabulary 안에 등장하는 단어들. 중복 허용. 어떤 corpus를 NLP 알고리즘의 훈련 데이터로 하기 위해서는 우선 그 corpus를 token 단위로 나누는 전처리가 필요하다. (이를 tokenizing이라 한다.)

- type: 한 vocabulary 안에 등장하는 단어들의 종류(=중복 비허용). 수억 개의 token으로 이루어진 corpus라 하더라도 그 corpus를 구성하는 type의 개수는 수만개 이하일 수 있다.



### 2. 각 언어의 tokenizing

#### 1) 중국어

\- 중국어는 한 문장을 구성하는 모든 글자가 각각 단어로서 의미를 갖는 한편으로 여러 글자가 모여 하나의 의미를 갖기도 한다. 여러 글자가 모여 만들어지는 단어를 token으로 분류하면 중국어를 사용하는 모델이 학습해야 하는 vocabulary의 수가 지나치게 늘어날 수 있어 보통은 글자 하나를 token으로 분류하는 편이다.

#### 2) 한국어

\- 띄어쓰기 문제도 있고, 한 어절 안에 여러 개의 형태소(뜻을 가진 최소 말 단위)가 있기 때문에 tokenizing이 어렵다. 어절이나 단어 단위가 아니라 그보다 더 작은 단위(subword)의 tokenizing이 필요하다.