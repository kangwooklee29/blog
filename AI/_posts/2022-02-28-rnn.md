---
title: RNN
---

### 1. RNN(recurrent neural network)

\- 시간에 따라 값이 변하는 순차데이터를 처리하는 신경망을 RNN이라 한다. 


### 2. 순차데이터의 표현

\- 일반적으로 벡터를 통해 표기한다. 입력 데이터의 크기는 그 데이터가 담고 있는 시간 길이에 비례하여 증가한다.

\- 문장을 벡터로 표현하는 방법에는 다음과 같은 방법이 있다.


- bag of words: 단어의 빈도수순으로 벡터의 각 성분에 단어를 대응시키고, 문장을 사전 크기와 같은 크기의 벡터로 표현. 각 문장이 담고 있는 전체 단어 집합 간 유사성을 판단할 수는 있으나, 시간 정보를 담을 수 없어 문장의 표현에는 부적절. 

- one hot code: 각 열벡터의 크기가 사전의 크기와 동일하고 단어를 one hot code로 표현하는, 문장에 포함된 단어 길이만큼의 벡터로 표현. 단어 하나를 표현하는 데 사전 크기만큼의 벡터를 사용해야 한다는 점은 공간 낭비이고, 단어 간 유사성을 판단할 수 없다.

- word embedding: 단어 사이의 관계를 분석하여, 단어 집합을 단어 사이 관계 정보가 담긴 새로운 공간으로 변환한다. 이 과정에서 단어를 표현하는 벡터의 차원은 사전의 크기보다 훨씬 작은 크기로 줄어든다. word2vec이 word embedding 방법으로 단어를 벡터로 변환하는 대표적인 방법이다. 


### 3. 순차데이터의 특성

- 특징이 나타나는 순서가 중요하다.

- 각 샘플마다 길이가 다르다.

- 한 데이터 안에서도 앞에 나오는 데이터와 뒤에 나오는 데이터 사이에 문맥 의존성이 있다.


### 4. RNN이 가져야 하는 필수 기능

- 시간: 순서대로 한 번에 하나의 특징이 나열된다.

- 가변 길이: 신경망에 샘플의 개수만큼 hidden layer가 있어야 하며, 샘플의 길이가 변하는 만큼 hidden layer의 개수도 변할 수 있어야 한다.

- 문맥 의존성: 이전에 입력된 특징의 내용을 나중에 입력된 특징을 처리하면서 함께 활용할 수 있어야 한다.


### 5. RNN의 구조

\- 대부분이 CNN과 구조가 유사하나, hidden layer 부분에 recurrent edge(한 hidden layer에서 나온 결과가 다시 그 hidden layer의 입력이 되는 edge)가 존재한다는 점이 다르다. RNN은 이러한 구조를 가짐으로써 데이터의 시간, 가변길이, 문맥의존성을 모두 처리할 수 있다.

\- 예를 들어, recurrent edge가 있는 부분에서 입력 데이터의 t초에 해당하는 데이터를 처리할 때는 입력 데이터의 t초에 해당하는 데이터와 t-1초에 해당하는 데이터를 그 hidden layer를 통해 처리한 결과값을 모두 그 hidden layer의 입력으로 하여 결과값을 얻는다.

\- 입력과 출력의 개수에 따라 RNN의 구조가 다음과 같이 나뉘기도 한다.

- 하나의 입력, 여러 개의 출력: image captioning 문제가 이와 같은 RNN 모델을 사용한다.

- 여러 개의 입력, 여러 개의 출력

  - 여러 개의 입력이 하나의 출력이 되고, 그 하나의 출력이 다시 RNN의 입력이 돼 다시 여러 개의 출력을 내놓는 RNN: transformer가 이와 같은 구조를 갖는다.

- 여러 개의 입력, 하나의 출력

