

### 1. 이미지 간 유사도 평가 방법

#### 1) L1 distance

$$
d_1 (I_1, I_2) = \sum_P \left| I_1^P - I_2^P \right|
$$

\- 크기가 같은 이미지의 같은 위치에 있는 각 픽셀이 갖는 값의 차이 절대값을 단순 합하여 이를 두 이미지의 L1 distance로 정의한다.

\- L1 distance는 좌표계가 달라지면 값도 달라진다는 문제가 있는데, 좌표계가 달라질 일 없는 문제에서라면 L1 distance가 L2 distance보다 좋은 결과를 낼 수도 있다.


#### 2) L2 distance

$$
d_2 (I_1, I_2) = \sqrt{\sum_P ( I_1^P - I_2^P )^2}
$$

\- 크기가 같은 이미지의 같은 위치에 있는 각 픽셀이 갖는 값의 차이 제곱을 단순 합하여 이를 두 이미지의 L1 distance로 정의한다.

\- L2 distance는 L1 distance와 달리 좌표계가 달라져도 값이 달라지지 않는다. 요소들 간 의미를 잘 모르는 상황이라면 L2 distance가 더 유용할 수 있다. 



### 2. k-NN(nearest neighbor) classifier


#### 1) NN classifier

\- 다음 두 단계로 동작한다.

(1) 훈련 단계: 특별한 작업을 수행하지 않고, 입력된 이미지의 클래스를 단지 기억하기만 한다. 단순히 메모리 위치 지정만 해도 되므로 시간복잡도는 O(1)로 구현할 수도 있다.

(2) 예측 단계: 방금 입력된 이미지를 기존 기억하던 이미지 중 가장 유사한 이미지의 클래스로 분류한다. L1 distance로 유사도를 평가할 경우 시간복잡도는 O(n)이다.

\- 실제 분류문제에서는 훈련하는 데 오래 걸려도 예측은 빠른 시간 내에 해결할 것을 요하며, 실제로 CNN 등은 그러한 구조를 갖고 있다. 그러나 NN classfier는 정반대다. 


#### 2) k-NN classifier 개요

\- 입력 이미지와 유사도가 낮은 이웃 이미지를 k개 찾아서 이들끼리 투표를 해 분류를 결정하는 알고리즘. 투표 방식은 여러 방식이 있을 수 있으나, 실제로는 최다득표로 분류하는 게 가장 잘 동작한다.

\- 컴퓨터 비전에서는 이미지를 여러 관점에서 볼 수 있어야 한다. 이미지 자체를 공간상의 한 점으로 볼 수도 있고, 이미지의 각 픽셀을 공간상의 한 점으로 보고 이미지를 이들의 집합으로 볼 수도 있다. 

\- 분류문제에서 k-NN classifier는 정답률이 높지 않아 아주 좋은 알고리즘은 아니다. 

\- k-NN classifier는 적절한 유사도 평가 방법을 사용하면 컴퓨터 비전뿐 아니라 어떤 분야에서도 사용할 수 있어, 새로운 분야에서 간단히 시도해보기 좋은 알고리즘이다. 그러나 사실 성능이 그렇게 뛰어난 방법은 아니며, 특히 컴퓨터 비전 분야에서는 잘 쓰이지 않는 방법인데 다음 이유 때문이다.

(1) 예측에 너무 오랜 시간이 걸린다.

(2) 이미지에 대해 k-NN을 사용하려면 L1/L2 distance를 사용해야만 하는데, L1/L2 distance는 이미지의 '지각적' 유사도를 평가하는 데 그닥 적절치 못하다. (약간의 변형으로 완전히 다른 이미지가 되기도 하고, 큰 변형에도 불구하고 이미지의 내용은 거의 달라지지 않는 경우도 있는데 L1/L2 distance로 보면 이 둘이 거의 같은 유사도를 갖는 것으로 평가된다.)

(3) 훈련 단계에서 쓸 데이터셋이 전체 공간을 모두 조밀하게 커버할 정도로 충분해야 하는데, 차원수가 커지면서 필요로 하는 데이터셋의 크기도 기하급수적으로 커진다. 이러한 데이터셋을 확보하는 건 비현실적이다. 


#### 3) hyperparameter 결정 방법

\- 어떤 k를 사용할지, 어떤 유사도 평가 방법을 사용할지 같은 요소는 훈련 단계에 들어가기 전에 결정해야 하는 요소로서, 이러한 요소를 hyperparameter라 한다. 이들을 결정하는 문제는 어떤 문제를 풀 것인가에 달렸다. 간단하게 생각하면, 같은 데이터를 두고 여러 hyperparameter를 적용해 보아 최적 결과를 내는 hyperparameter를 선택하면 된다. 

(1) 훈련 단계 때 사용한 데이터를 가장 잘 분류하는 hyperparameter를 선택

- 훈련 단계 때 사용된 데이터를 입력으로 넣는다면 k=1이 k>1보다 정확도가 높아 k=1이 선택될 것. 그러나 훈련 단계 때 사용된 적 없는 새로운 데이터가 입력으로 들어온다면 k=1의 정확도는 떨어질 것이고, ML에선 훈련 단계 때 사용되지 않은 데이터가 입력으로 들어오는 상황이 더 중요.

(2) 전체 데이터셋 중 일부만 훈련 단계에서 사용하고, 나머지는 빼두었다가 최적 hyperparameter를 결정하기 위한 테스트용 이미지로 활용

- hyperparameter 결정 단계에서 사용한 데이터셋이 실제로 앞으로 입력할 이미지와 비슷한 구성을 갖는다는 보장이 없기 때문에 이 역시 좋은 방법이라 할 수 없다. hyperparameter 결정 단계에서 사용한 데이터셋에만 최적의 결과를 내는 hyperparameter에 불과할 수 있다.

(3) 전체 데이터셋을 셋으로 나누어, 하나는 훈련 단계에서 사용하고 하나는 hyperparameter 결정 단계(검증 단계)에서 사용하고 마지막 하나는 알고리즘의 성능을 딱 1번 테스트하는 목적으로만 사용

- 분류 알고리즘 문제에서는 검증 단계와 테스트 단계를 구분하는 게 중요.

(4) 먼저 테스트용 데이터를 따로 빼두고, 나머지 데이터를 n개 그룹으로 나누어 한번은 1번 그룹을 검증 데이터로 사용하고 나머지를 훈련 데이터로 사용, 한번은 2번 그룹을 검증 데이터로 사용하고 나머지를 훈련 데이터로 사용, ... 해서 각각이 내는 최적 결과를 비교('교차검증')

- 훈련 데이터로 적은 데이터를 사용하는 경우에는 흔히 쓰이는 방식이지만, 대량의 데이터로 학습을 하는 딥러닝 분야에서는 잘 쓰이지 않는 방법.



### 3. linear classification


\- NN(neural network)은 다양한 컴포넌트로 구축한 것으로서 CNN을 구성한다. NN을 레고 블럭으로, CNN을 그 레고 블럭을 조합해 만든 결과물로 각각 비유할 수 있다. (예를 들어, image captioning은 단순히 CNN에 RNN이라는 언어학습 알고리즘을 결합해 학습시켜 문제를 해결한다.) 그리고 linear classification은 CNN의 기반 알고리즘으로서, 간단하지만 중요한, 가장 기본이 되는 블럭 중 하나다. 따라서 linear classification의 동작 원리를 이해하는 것은 매우 중요하다.

\- k-NN classifier와 대비하여 CNN 같은 알고리즘을 흔히 paramtric model이라 한다. k-NN classifier는 학습한 데이터를 곧이곧대로 저장만 해뒀다가 입력이 들어오면 그 데이터를 저장된 데이터 전체와 비교하는 작업을 수행하는데, parametric model은 학습한 데이터를 가지고 가중치 행렬을 만들어(흔히 W라 한다) 그 행렬과 입력 데이터를 연산해 그 결과값으로 분류 작업을 수행한다.

$$

f(x, W) = Wx = 
\begin{bmarix}
w_{1, 1} & \cdots & w_{1, n} \\
\vdots & \ddots & \vdots \\
w_{m, 1} & \cdots & w_{m, n} \end{bmatrix}

\begin{bmatrix}

x_1 \\
\vdots \\
x_n

\end{bmatrix}

= \begin{bmatrix}

\sum_i=1^n w_{1, i} x_i \\

\vdots \\

\sum_i=1^n w_{m, i} x_i \end{bmatrix}

$$

\- linear classification은 parametric model의 가장 단순한 형태로, 입력 데이터가 총 \\(n\\)개의 픽셀로 이루어져 있고 \\(m\\)개의 클래스로 분류해야 한다면 \\(m \times n\\) 크기의 가중치 행렬을 만들고 입력 데이터를 \\(n\\)행의 열벡터로 만들어 이 둘을 단순히 곱해(또는 그 결과값에 보정 차원에서 특정 열벡터를 합산해) 그 결과값으로 분류 작업을 수행하는 알고리즘이다. (결과값이 가장 큰 행에 해당하는 클래스로 분류한다.) 

\- 여기서 가중치 행렬의 행벡터와 입력 데이터 열벡터를 내적하는 것을 입력 데이터의 각 클래스에 대한 유사도를 계산하는 과정으로 볼 수 있다. 이렇게 생각할 때, 가중치 행렬의 행벡터 하나하나는 각 클래스의 주요 특징들을 모아놓은 하나의 대표 모델(흔히 템플릿이라 한다)이라고 생각할 수 있다. linear classification은 이처럼 특정 클래스와의 유사도를 계산하는 데 있어서 딱 하나의 템플릿과만 유사도를 측정하기 때문에 성능에 한계가 있지만, 보다 복잡한 신경망 알고리즘에서는 보다 더 많은 템플릿과 유사도를 측정하므로 이러한 문제는 극복 가능하다.

\- 입력 데이터와 그에 대응되는 linear classification 결과값으로 좌표공간에 점을 찍을 수 있다고 생각하면, 같은 클래스로 분류되는 입력 데이터들은 좌표공간 상에서 비슷한 영역에 있으며 그 경계는 linear한 경계를 기준으로 나뉘게 된다. 

\- linear classification은 parity 문제나 multi modes 문제를 해결하는 데 어려움이 있다. 이들 문제는 예를 들어 입력 데이터들이 실제로는 같은 클래스로 분류돼야 함에도 좌표공간상에서 서로 다른 영역에 밀집되어 linear한 경계로는 이들을 같은 클래스로 분류하지 못하는 경우에 해당한다. 