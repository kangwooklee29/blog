### 1. 손실함수

\- linear classification에서 좋은 결과를 내기 위해서는 보다 나은 가중치 행렬을 얻어야 한다. 이를 위해 우선 가중치 행렬의 좋고 나쁨을 정량화할 수 있어야 하는데, 보통 손실함수(loss function)의 값을 이용해 가중치 행렬의 좋고 나쁨을 수치로 정량화한다. 그리고 손실함수를 이용해서 최선의 가중치 행렬을 구하는 과정을 최적화(optimization)이라 한다.

\- 손실함수는 주어진 데이터에 관한 **예측함수 함수값 열벡터**와 **실제 그 데이터의 classification에 관한 열벡터**를 통해 함수값을 계산하는 함수이다. 구체적으로, 가중치 행렬 \\(W\\)와 입력 데이터 열벡터 \\(x\\), 그 입력 데이터가 실제 갖는 classification에 관한 열벡터 \\(y\\)가 주어지며 또 \\(x, W\\)를 인자로 하는 예측 함수 \\(f\\)로 얻은 함수값 열벡터 \\(f(x, W)\\)가 있다 할 때, 손실함수 \\(L\\)은 \\(f(x, W)\\)와 \\(y\\)를 인자로 하는 함수이다.


### 2. multi-class SVM

\- 손실함수에 관한 구체적인 사례로 multi-class SVM이라는 손실함수가 있으며, 입력 데이터 열벡터 \\(x\\)와 그에 대한 classification 열벡터 \\(y\\)에 대해 그때의 multi-class SVM 손실함수 \\(L\\)은 다음과 같이 정의된다.

$$
L(f(x, W), y) = \sum_{i \ne y} max(0,  s_i - s_y + 1) \\
$$


- \\(s_i\\): 예측함수의 함수값 열벡터 \\(f(x, W)\\)의 \\(i\\)행의 성분을 뜻한다. 

- \\(s_y\\): \\(x\\)에 대응되는 열벡터 \\(y\\)는 특정 행의 성분만 0이 아니고 나머지는 모두 0인데, \\(s_y\\)는 \\(f(x, W)\\)의 행 중 \\(y\\)의 0이 아닌 행과 같은 행의 성분을 뜻한다.

- \\(max(0, s_i - s_y + 1)\\): \\(s_i\\)가 \\(s_y\\)보다 크거나, 작더라도 그 차이가 1보다 작다면 '그 차이 + 1'이라는 값을 손실함수 값을 계산할 때 합산할 것임을 뜻한다. \\(s_i\\)가 \\(s_y\\)보다 작더라도 그 차이가 1보다 작게 계산된다는 것은 그만큼 **둘 사이 차이가 아주 작다**는 것인데, 이를 통해 그 예측함수가 그만큼 좋은 함수가 아닐 수 있다고 추측할 수 있다.

- \\(\sum_{i \ne y}\\): \\(f(x, W)\\)의 \\(s_y\\)가 있는 행을 제외한 **모든 행**에 대하여 수식을 계산해서 그 값을 합하도록 함을 뜻한다.


\- 보통 초기에는 \\(W\\)의 각 성분이 0에 매우 가깝고 각 성분 사이 차이가 그리 크지 않으므로, \\(s_i - s_y\\)의 차이 또한 그리 크지 않아 손실함수 값이 작게 나오기 마련이다. 만약 이 손실함수에서 \\(s_i - s_y\\) 뒤에 1을 더하지 않았다면 아주 초기에 손실함수값이 0이 나와 더 나은 손실함수를 찾지 않게 했을 것이다. 그러나 이 손실함수는 그런 경우에 대비해 1을 더해줌으로써 \\(W\\)의 모든 성분 사이 차이가 그리 크지 않은 경우에는 손실함수 값이 최소한 '\\(W\\)의 행의 개수 - 1'이 되게 하여 계속 더 나은 \\(W\\)를 찾을 수 있게 한다.

\- 만약 어떤 \\(W\\)가 손실함수 값을 0으로 만든다면, \\(2W\\) 또한 손실함수 값을 0으로 만든다. 