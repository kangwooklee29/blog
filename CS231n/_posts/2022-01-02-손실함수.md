### 1. 손실함수

\- linear classification에서 좋은 결과를 내기 위해서는 보다 나은 가중치 행렬을 얻어야 한다. 이를 위해 우선 가중치 행렬의 좋고 나쁨을 정량화할 수 있어야 하는데, 보통 손실함수(loss function)의 값을 이용해 가중치 행렬의 좋고 나쁨을 수치로 정량화한다. 그리고 손실함수를 이용해서 최선의 가중치 행렬을 구하는 과정을 최적화(optimization)이라 한다.

\- 손실함수는 주어진 데이터에 관한 **예측함수 함수값 열벡터**와 **실제 그 데이터의 classification에 관한 열벡터**를 통해 함수값을 계산하는 함수이다. 구체적으로, 가중치 행렬 \\(W\\)와 입력 데이터 열벡터 \\(x\\), 그 입력 데이터가 실제 갖는 classification에 관한 열벡터 \\(y\\)가 주어지며 또 \\(x, W\\)를 인자로 하는 예측 함수 \\(f\\)로 얻은 함수값 열벡터 \\(f(x, W)\\)가 있다 할 때, 손실함수 \\(L\\)은 \\(f(x, W)\\)와 \\(y\\)를 인자로 하는 함수이다.


### 2. multi-class SVM

#### 1) 손실함수

\- 손실함수에 관한 구체적인 사례로 multi-class SVM이라는 손실함수가 있으며, 입력 데이터 열벡터 \\(x\\)와 그에 대한 classification 열벡터 \\(y\\)에 대해 그때의 multi-class SVM 손실함수 \\(L\\)은 다음과 같이 정의된다.

$$
L(f(x, W), y) = \sum_{i \ne y} max(0,  s_i - s_y + 1) \\
$$


- \\(s_i\\): 예측함수의 함수값 열벡터 \\(f(x, W)\\)의 \\(i\\)행의 성분을 뜻한다. 흔히 \\(i\\)행 레이블에 해당하는 '점수(score)'라고 부르기도 한다.

- \\(s_y\\): \\(x\\)에 대응되는 열벡터 \\(y\\)는 특정 행의 성분만 0이 아니고 나머지는 모두 0인데, \\(s_y\\)는 \\(f(x, W)\\)의 행 중 \\(y\\)의 0이 아닌 행과 같은 행의 성분을 뜻한다.

- \\(max(0, s_i - s_y + 1)\\): \\(s_i\\)가 \\(s_y\\)보다 크거나, 작더라도 그 차이가 1보다 작다면 '그 차이 + 1'이라는 값을 손실함수 값을 계산할 때 합산할 것임을 뜻한다. \\(s_i\\)가 \\(s_y\\)보다 작더라도 그 차이가 1보다 작게 계산된다는 것은 그만큼 **둘 사이 차이가 아주 작다**는 것인데, 이를 통해 그 예측함수가 그만큼 좋은 함수가 아닐 수 있다고 추측할 수 있다.

- \\(\sum_{i \ne y}\\): \\(f(x, W)\\)의 \\(s_y\\)가 있는 행을 제외한 **모든 행**에 대하여 수식을 계산해서 그 값을 합하도록 함을 뜻한다.


\- 보통 초기에는 \\(W\\)의 각 성분이 0에 매우 가깝고 각 성분 사이 차이가 그리 크지 않으므로, \\(s_i - s_y\\)의 차이 또한 그리 크지 않아 손실함수 값이 작게 나오기 마련이다. 만약 이 손실함수에서 \\(s_i - s_y\\) 뒤에 1을 더하지 않았다면 아주 초기에 손실함수값이 0이 나와 더 나은 손실함수를 찾지 않게 했을 것이다. 그러나 이 손실함수는 그런 경우에 대비해 1을 더해줌으로써 \\(W\\)의 모든 성분 사이 차이가 그리 크지 않은 경우에는 손실함수 값이 최소한 '\\(W\\)의 행의 개수 - 1'이 되게 하여 계속 더 나은 \\(W\\)를 찾을 수 있게 한다.

#### 2) regularization

\- 만약 어떤 가중치 행렬 \\(W\\)가 손실함수 값을 0으로 만든다면, 그\\(W\\)에 상수배를 한 가중치 행렬 또한 손실함수 값을 0으로 만든다. 즉, 손실함수 값을 0으로 만드는 가중치 행렬의 개수는 무수히 많다. 그러나 그 모든 가중치 행렬이 실제 예측에 있어 모두 똑같은 정확도를 갖는 것은 아니다. 손실함수만을 통해 구한 가중치 행렬은 이를 구하는 과정에서 대입한 입력 데이터(훈련 데이터)에 대해서만 최적화돼있는 경우가 많고, 따라서 이렇게 구한 가중치 행렬이 훈련 데이터에서 사용하지 않은 전혀 다른 테스트 입력에 대해서도 정확한 결과를 내리라는 보장은 없다. 이 문제를 해결하기 위해 이제부터는 다음과 같은 모델을 생각한다.

$$
L(W) = {1 \over {N}} \sum_{i=1} ^{N} L_i(f(x_i, W), \, y_i) + \lambda R(W) \\
$$
(단, \\(x_i\\)는 훈련 데이터이고 \\(y_i\\)는 그것의 classification 열벡터이다.)

\- 여기서 \\(L(W)\\)는 가중치 행렬 \\(W\\)가 주어질 때 훈련 데이터셋 \\((x_1, y_1), \cdots, (x_n, y_n)\\)에 대한 손실을 계산하는 함수이고, 여기서 앞항은 그 \\(W\\)에 대한 각 훈련 데이터의 손실함수값을 계산해 이들의 평균을 구한 것이다. 그리고 여기서 뒷항은 이 '가능한 여러 개의 \\(W\\) 중 \\(L(W)\\)를 최소로 하는 \\(W\\)를 선택하는 문제'에서 **특정 사례에만 최적화된 \\(W\\)를 제거**하기 위해 추가된 항이다. 이와 같은 의도로 \\(W\\)에 관한 항을 더하는 것을 정규화(regularization)라 하며, 정규화 항(\\(R(W)\\))을 어떻게 선택할 것인지와 그에 대해 얼만큼의 가중치(\\(\lambda\\)를 둘지가 문제가 된다.

\- 가장 흔히 쓰이는 정규화항은 \\(W\\)의 각 성분을 제곱한 후 모두 더한 것(\\(\sum_k \sum_l W_{k,l} ^2 \\))으로, L2 정규화라 한다. L2 정규화는 \\(W\\)의 어느 한 성분이 더 크고 나머지 성분이 아주 작은 \\(W\\)보다 **여러 성분이 고루 작은** \\(W\\)를 더 선호하는 경향이 있다.


### 3. softmax loss(multinomial logistic regression)

\- \\(C\\)행의 가중치벡터 \\(W\\)와 이미지 데이터 열벡터 \\(x_i\\) (단, \\(i=1, \cdots, n\\))에 대하여 예측함수 \\(f(x_i, W)\\)의 각 행을 \\(s_k\\) (단, \\(k=1, \cdots, C)\\)이라 할 때, \\(x_i\\)가 \\(f\\)에 의해 \\(f\\)의 각 행에 해당하는 레이블로 분류될 확률 \\(P(Y=y_i \| X=x_i )\\)는 다음과 같이 쓸 수 있다.

$$
P(Y=k | X=x_i ) = { e^{s_k} \over \sum_j e^{s_j}}
$$

\- 이 \\(P(Y=y_i \| X=x_i )\\)를 softmax 함수라 하며, multiclass-SVM과 달리 예측함수의 각 성분에 '확률'이라는 의미를 부여한 것이다. (예측함수의 각 성분은 음수도 나올 수 있으나 음의 확률이라는 것은 존재하지 않기 때문에, 각 성분을 \\(e\\)의 지수로 취하여 양수값을 만들어낸 것이다.)

\- 한편 multiclass-SVM에서 보았듯 loss는 예측함수가 정확하면 0이고 부정확하면 무한히 커진다. 이러한 특성에 부합하도록 softmax loss를 \\(-log P(Y=y_i \| X=x_i )\\)로 정의할 수 있다. 즉 softmax loss를 '정확히 분류할 확률의 음수로그'라고 하면, 정확히 분류했을 땐 loss가 0이 되고 정확히 분류하지 못할 확률이 0에 가까워질수록 loss가 무한대에 가까워지는 식을 얻을 수 있다.

\- W의 값이 모두 0에 가깝게 작다면 \\(f(x_i, W)\\)의 각 성분은 모두 비슷한 값을 갖게 될 것이고, 그렇게 되면 softmax 함수의 값은 모두 \\({1 \over C\\)로 동일 할 것이다. 따라서 이때의 softmax loss는 \\(log C\\)가 된다.

\- multiclass-SVM의 경우 hinge 형태의 그래프를 갖기 때문에 (정답 레이블의 점수(\\(s_{y_i}\\))가 오답 레이블의 점수(\\(s_j\\)보다 1 이상 크기만 하다면, 즉 \\(s_j - s_{y_i} + 1 \leq 0 \\) 이면) 정답 레이블 점수가 약간 달라지더라도 loss는 변하지 않았으나, softmax의 경우 **정답 레이블의 점수가 약간만 달라져도 loss는 반드시 변하게 돼있다.** 실제로는 이것이 multiclass-SVM과 아주 큰 차이를 나타내지는 않으나, 함수의 형태적 특성에 이러한 차이가 있다는 것을 알아둘 필요는 있다.



### 4. optimization

\- 