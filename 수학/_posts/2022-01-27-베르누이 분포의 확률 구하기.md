### 1. 베르누이 분포

\- 시행의 결과로 나오는 결과가 딱 둘뿐인 사건의 확률분포를 베르누이 분포라 한다. 그 확률변수 \\(x\\)가 0 또는 1의 값만 갖는데 각각이 나올 확률 \\(\mu, 1-\mu\\)는 모른다면, \\(p(x=0 \mid \mu) = \mu, p(x=1 \mid \mu) = 1-\mu\\)로 쓸 수 있다. 이를 \\(\mathrm{Bern}(x \mid \mu) = \mu^{(1-x)}(1-\mu)^x\\)로 쓸 수 있다.

\- 베르누이 분포의 기댓값 \\(\mathbb{E}(x) = \mu\\), 분산 \\(\mathrm{var}(x) = \mu(1-\mu)\\)이다.

### 2. 최대우도법으로 베르누이 분포의 확률 구하기

\- 예를 들어 베르누이 분포를 갖는 확률변수 \\(x\\)에 대해 1회 시행의 결과로 0이 나올 확률 \\(\mu\\)를 알지 못하는데 \\(N\\)회 시행해 그 결과 \\(D = \left\\{ x_1, \cdots, x_n \right\\} \\) 및 그 결과에서 0이 나온 횟수 \\(m\\)을 알고 있을 때, 최대우도법으로 그 결과를 보고 \\(\mu\\)를 추론할 수 있다.

\- 우도함수 \\(p(D \mid \mu) = \prod_{n=1}^N \mu^{1-x_n} (1-\mu)^{x_n}\\) 이다. \\(x_n\\)은 \\(m\\)회만 1의 값을 가지므로, \\(p(D \mid \mu)\\)를 최대로 하는 \\(\mu= { m \over N } \\)임을 알 수 있다.


### 3. 베이즈 정리로 베르누이 분포의 확률 구하기

#### 1) 이항분포

\- 예를 들어 베르누이 분포를 갖는 확률변수 \\(x\\)에 대해 1회 시행의 결과로 0이 나올 확률 \\(\mu\\)를 알지 못하는데 \\(N\\)회 시행해 그 결과에서 0이 나오는 횟수를 \\(m\\)이라 할 때, '그 결과에서 0이 나오는 횟수가 \\(m\\)'이라는 확률변수가 갖는 확률분포를 이항분포라 하며 그때의 확률을 다음과 같이 쓸 수 있다.

$$
\mathrm{Bin}(m \mid N, \mu) = {N \choose \mu} \mu^{N-m} (1-\mu)^m
$$

\- 이항분포의 기댓값 \\(\mathbb{E}(m) = N\mu\\), 분산 \\(\mathrm{var}(m) = N\mu(1-\mu)\\)이다.

\- 베이즈 정리로 베르누이 분포의 확률을 직접 구하기는 어려운데, \\(N\\)회 시행한 결과 \\(D = \left\\{ x_1, \cdots, x_n \right\\}\\)는 확률변수가 \\(N\\)개나 되기 때문이다. 그러나 그 베르누이 분포를 기초로 구한 이항분포의 확률은 그 베르누이 분포의 확률과 일치하면서, 확률변수의 개수는 하나로 줄어들어 베이즈 정리를 적용하기가 훨씬 쉬워진다.


#### 2) 베타분포

\- 확률변수 \\(\mu\\)가 0 이상 1 이하의 실수를 갖는 어떤 확률분포가 임의의 실수 \\(a, b\\)에 대해 다음과 같은 확률을 가질 때, 이 확률분포를 베타분포라 한다.

$$
\mathrm{Beta}(\mu \mid a, b) = { {\Gamma(a+b)} \over {\Gamma(a) \Gamma(b)} } \mu ^ {a-1} (1-\mu)^{b-1}
$$

\- 베타분포의 기댓값 \\(\mathbb{E}(\mu) = {a \over {a+b}}\\), 분산 \\(\mathrm{var}(\mu) = {ab \over {(a+b)^2 (a+b+1)}}\\)이다.


#### 3) 베이즈 정리로 베르누이 분포의 확률 구하기

\- 베르누이 분포를 갖는 확률변수 \\(x\\)에 대해 1회 시행의 결과로 0이 나올 확률 \\(\mu\\)를 알지 못하는데 \\(N\\)회 시행해 그 결과에서 0이 나오는 횟수 \\(m\\)을 알 때 0이 \\(m\\)번 나올 확률이 \\(\mathrm{Bin}(m \mid N, \mu)\\)라 하고(우도), 임의의 \\(a, b\\)를 선택하여 추론해둔 \\(\mu\\)의 확률분포를 \\(\mathrm{Beta}(\mu \mid a, b)\\)라 할 때(사전확률), \\(N\\)회 시행한 결과를 반영하여 추론한 \\(\mu\\)의 확률분포(사후확률)는 다음과 같다.

$$
\begin{matrix}

p(\mu \mid m, a, b) &=& { {\mathrm{Bin}(m \mid N, \mu)\mathrm{Beta}(\mu \mid a, b)} \over {\int_{0}^1 \mathrm{Bin}(m \mid N, \mu)\mathrm{Beta}(\mu \mid a, b) d\mu} }\\

&=& { {\Gamma(N+a+b)} \over {\Gamma(m+a)\Gamma(N-m+b)} } \mu^{m+a-1} (1-\mu)^{N-m+b-1}

\end{matrix}
$$



### 4. 확률변수가 벡터인 베르누이 분포의 확률 구하기

#### 1) 최대우도법으로 구하기

\- 예를 들어 확률변수가 \\(K\\)-벡터 \\(\mathbf{x}\\)이고, \\(\mathbf{x}\\)의 각 성분 \\(x_k\\)은 0 또는 1인데 1은 하나뿐이고 나머지는 모두 0인 베르누이 분포가 있고 이때 \\(x_k\\)가 0일 확률 \\(\mu_k\\)(\\(k=1, \cdots, K\\))를 알지 못한다 하자. 

\- 이때 \\(p(\mathbf{x} \mid \mathbf{\mu}) = \prod_{k=1}^K \mu_{k}^{x_k}\\) 이고, \\(\mathbf{x}\\)의 기댓값 \\(\mathbb{E}(\mathbf{x} \mid \mathbf{\mu}) = \sum_{\mathbf{x}}p(\mathbf{x} \mid \mu) = \mathbf{\mu}\\) 이다.

\- \\(\mathbf{x}\\)를 \\(N\\)번 관찰한 결과 \\(\mathcal{D} = \left\\{\mathbf{x}_ 1, \cdots, \mathbf{x}_ N \right\\}\\)라 할 때, \\(\mathbf{\mu}\\)에 관한 우도함수 \\(p(\mathcal{D} \mid \mathbf{\mu}) = \prod_{k=1}^K \mu_{k}^{\sum_{n}x_{n,k} }\\) 이다.

\- 최대우도법으로 \\(p(\mathcal{D} \mid \mathbf{\mu})\\)을 최대화시키는 \\(\mathbf{\mu}\\)을 구할 수 있다. 정리하면 \\(\mu_k = {1 \over N} \sum_{n}x_{n,k}\\) 을 얻는다.


#### 2) 베이즈 정리로 구하기

\- \\(m_k = \sum_{n}x_{n,k}\\)라 할 때, \\(\mathbf{\mu}\\)와 \\(N\\)이 주어졌을 때 \\(m_1, \cdots, m_K\\)의 분포를 다항분포라 하며, 다음과 같이 쓴다.

$$

\mathrm{Mult}(m_1, \cdots, m_K \mid \mu, N) = {N \choose {m_1, \cdots ,m_K} } \prod_{k=1}^K \mu_{k}^m_k

$$

\- 확률변수인 \\(K\\)-벡터 \\(\mathbf{\mu}\\)의 각 성분이 0 이상 1 이하의 실수를 갖는 어떤 확률분포가 임의의 실수를 성분으로 하는 \\(K\\)-벡터 \\(\mathbf{\alpha} = \begin{bmatrix} \alpha_1 & \cdots & \alpha_K \end{bmatrix}^T\\)에 대해 다음과 같은 확률을 가질 때, 이 확률분포를 디리클레 분포라 한다.

$$
\mathrm{Dir}(\mathbf{\mu} \mid \mathbf{\alpha}) = { {\Gamma(\sum_{k=1}^K \alpha_k)} \over {\Gamma(\alpha_1) \cdots \Gamma(\alpha_K)} } \prod_{k=1}^K \mu_{k} ^ {\alpha_k-1} 
$$

\- \\(\mathbf{\mu}\\)의 사후확률 \\(p(\mathbf{\mu} \mid \mathcal{D}, \mathbf{\alpha})\\)는 다음과 같이 구해진다.

$$
\begin{matrix}
p(\mathbf{\mu} \mid \mathcal{D}, \mathbf{\alpha}) &=& \mathrm{Dir}(\mathbf{\mu} \mid \mathbf{\alpha} + \begin{bmatrix} m_1 & \cdots & m_K \end{bmatrix}^T) \\

&=& { {\Gamma(\sum_{k=1}^K \alpha_k+N)} \over {\Gamma(m_1+\alpha_1)\cdots \Gamma(m_K + \alpha_K)} } \prod_{k=1}^K \mu^{m_k+\alpha_k-1} 


\end{matrix}


$$