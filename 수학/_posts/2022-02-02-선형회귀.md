### 1. 선형 기저함수 모델

\- 입력 데이터로 \\(D\\)-벡터 \\(\mathbf{x} = \begin{bmatrix} x_1 & \cdots & x_D \end{bmatrix}^T \\)가 주어지고 파라미터로 \\((D+1)\\)-벡터 \\(\mathbf{w} = \begin{bmatrix} w_0 & \cdots & w_D \end{bmatrix}^T \\)가 주어진다 하자.

- 이때 \\(y(\mathbf{x}, \mathbf{w}) = w_0 + \displaystyle \sum_{j=1}^D w_j x_j\\)은 \\(\mathbf{w}, \mathbf{x}\\) 모두에 대해 선형인 모델이라 할 수 있다.

- 한편 비선형함수 \\(\phi_1(\mathbf{x}), \cdots, \phi_D(\mathbf{x})\\)에 대하여 \\(y(\mathbf{x}, \mathbf{w}) = w_0 + \displaystyle \sum_{j=1}^D w_j \phi_j(\mathbf{x})\\)라는 모델을 생각할 수 있다. 이 모델은 \\(\mathbf{x}\\)에 대해 비선형인 모델이라 할 수 있다. (단, 이 경우에도 이 모델은 \\(\mathbf{w}\\)에 대해서는 선형이다.)

  - 이 모델에서 \\(\phi_1(\mathbf{x}), \cdots, \phi_D(\mathbf{x})\\)를 기저함수라 한다.


### 2. 최대우도법으로 선형 기저함수 모델의 가중치 벡터 구하기

\- 앞서 다루었던 '분류문제의 선형 모델의 가중치 벡터 \\(\mathbf{w}\\)를 최대우도법으로 구하는 문제'에서 유추하여, 어떤 \\(N\\)-벡터 \\(\mathbf{x}_i\\)를 입력받으면 그에 대한 결과로 실수 \\(t_i\\)(\\(i = 1, \cdots, N\\))가 나와야 하는 훈련 데이터셋이 \\(\mathbf{w}^T \phi(\mathbf{x}_i)\\)를 평균, \\(\beta^{-1}\\)을 분산으로 하는 정규분포 \\(\mathcal{N}(t \mid \mathbf{w}^T \phi(\mathbf{x}_i), \beta^{-1}) \\)에서 추출된 \\(N\\)개의 임의의 표본이라고 보자. 

\- 이때 우도함수 \\(p(\mathbf{t} \mid \mathbf{x}_1, \cdots, \mathbf{x}_N, \mathbf{w}, \beta) =  \displaystyle \prod _ {n=1}^N \mathcal{N}(t_n \mid\mathbf{w}^T \phi(\mathbf{x}_i),  \beta^{-1}) \\) 이다.

  - 이 식에 양변 로그 취하고 또 다시 양변 gradient 취한 다음 정리하면 \\(\mathbf{w} _ {ML} = (\Phi^T\Phi)^{-1}\Phi^T \mathbf{t} \\) 를 얻는다. (단, \\(\Phi = \begin{bmatrix} \phi_0(\mathbf{x}_1) & \cdots & \phi _ {M-1}(\mathbf{x}_1) \\\ \vdots & \ddots & \vdots \\\ \phi_0(\mathbf{x}_N) & \cdots & \phi _ {M-1}(\mathbf{x}_N) \end{bmatrix} \\)) 이 식을 흔히 normal equation이라고도 부른다.


### 3. 선형 기저함수 모델에서 경사하강법으로 가중치 벡터 구하기

\- \\(\mathbf{w}' = \mathbf{w} - \eta \grad E_n\\)에서, 만약 에러함수가 제곱합 함수라면 \\(\mathbf{w}' = \mathbf{w} + \eta (t_n - \mathbf{w}^T \phi_n(\mathbf{x}_n)) \phi_n(\mathbf{x}_n) \\) 로 쓸 수 있다.