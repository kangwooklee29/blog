### 1. 선형 기저함수 모델

\- 입력 데이터로 \\(D\\)-벡터 \\(\mathbf{x} = \begin{bmatrix} x_1 & \cdots & x_D \end{bmatrix}^T \\)가 주어지고 파라미터로 \\((D+1)\\)-벡터 \\(\mathbf{w} = \begin{bmatrix} w_0 & \cdots & w_D \end{bmatrix}^T \\)가 주어진다 하자.

- 이때 \\(y(\mathbf{x}, \mathbf{w}) = w_0 + \displaystyle \sum_{j=1}^D w_j x_j\\)은 \\(\mathbf{w}, \mathbf{x}\\) 모두에 대해 선형인 모델이라 할 수 있다.

- 한편 비선형함수 \\(\phi_1(\mathbf{x}), \cdots, \phi_D(\mathbf{x})\\)에 대하여 \\(y(\mathbf{x}, \mathbf{w}) = w_0 + \displaystyle \sum_{j=1}^D w_j \phi_j(\mathbf{x})\\)라는 모델을 생각할 수 있다. 이 모델은 \\(\mathbf{x}\\)에 대해 비선형인 모델이라 할 수 있다. (단, 이 경우에도 이 모델은 \\(\mathbf{w}\\)에 대해서는 선형이다.)

  - 이 모델에서 \\(\phi_1(\mathbf{x}), \cdots, \phi_D(\mathbf{x})\\)를 기저함수라 한다.


### 2. 최대우도법으로 선형 기저함수 모델의 가중치 벡터 구하기

\- 앞서 다루었던 '분류문제의 선형 모델의 가중치 벡터 \\(\mathbf{w}\\)를 최대우도법으로 구하는 문제'에서 유추하여, 어떤 \\(N\\)-벡터 \\(\mathbf{x}_i\\)를 입력받으면 그에 대한 결과로 실수 \\(t_i\\)(\\(i = 1, \cdots, N\\))가 나와야 하는 훈련 데이터셋이 \\(\mathbf{w}^T \phi(\mathbf{x}_i)\\)를 평균, \\(\beta^{-1}\\)을 분산으로 하는 정규분포 \\(\mathcal{N}(t \mid \mathbf{w}^T \phi(\mathbf{x}_i), \beta^{-1}) \\)에서 추출된 \\(N\\)개의 임의의 표본이라고 보자. 

\- 이때 우도함수 \\(p(\mathbf{t} \mid \mathbf{x}_1, \cdots, \mathbf{x}_N, \mathbf{w}, \beta) =  \displaystyle \prod _ {n=1}^N \mathcal{N}(t_n \mid\mathbf{w}^T \phi(\mathbf{x}_i),  \beta^{-1}) \\) 이다.

  - 이 식에 양변 로그 취하고 또 다시 양변 gradient 취한 다음 정리하면 \\(\mathbf{w} _ {ML} = (\Phi^T\Phi)^{-1}\Phi^T \mathbf{t} \\) 를 얻는다. (단, \\(\Phi = \begin{bmatrix} \phi_0(\mathbf{x}_1) & \cdots & \phi _ {M-1}(\mathbf{x}_1) \\\ \vdots & \ddots & \vdots \\\ \phi_0(\mathbf{x}_N) & \cdots & \phi _ {M-1}(\mathbf{x}_N) \end{bmatrix} \\)) 이 식을 흔히 normal equation이라고도 부른다.

  - 규제화항을 추가한 에러함수가 \\(\displaystyle {1 \over 2} \sum_{n=1}^N \\{t_n \mathbf{w}^T \phi_n(\mathbf{x})\\}^2 + {\lambda \over 2} \mathbf{w}^T \mathbf{w}\\) 일 때 normal equation은 \\(\mathbf{w} _ {ML} = ( \lambda I + \Phi^T\Phi)^{-1}\Phi^T \mathbf{t} \\) 이다.


### 3. 선형 기저함수 모델에서 경사하강법으로 가중치 벡터 구하기

\- \\(\mathbf{w}' = \mathbf{w} - \eta \nabla E_n\\)에서, 만약 에러함수가 제곱합 함수라면 \\(\mathbf{w}' = \mathbf{w} + \eta (t_n - \mathbf{w}^T \phi_n(\mathbf{x}_n)) \phi_n(\mathbf{x}_n) \\) 로 쓸 수 있다.


### 4. 베이즈 정리와 선형회귀

\- 베이즈 정리를 이용하여, 데이터셋 \\(\mathbf{t} = \begin{bmatrix} t_1 & \cdots & t_N \end{bmatrix}^T \\)에 대한 파라미터 벡터 \\(\mathbf{w}\\)의 식을 쓸 수 있다. 먼저, \\(\mathbf{w}\\)의 사전확률 \\(p(\mathbf{w}) = \mathcal{N}(\mathbf{w} \mid \mathbf{m} _ 0, \mathbf{S} _ 0)\\)라 하자. 이때 우도 \\(p(\mathbf{t} \mid \mathbf{w}) = \displaystyle \prod _ {n=1}^N \mathcal{N}(t_n \mid \mathbf{w}^T \phi(\mathbf{x} _ n), \beta^{-1}) = \mathcal{N}(\mathbf{t} \mid \mathbf{\Phi} \mathbf{w}, \beta^{-1} \mathbf{I})\\) 이다.

\- 한편 \\(p(\mathbf{x}) = \mathcal{N}(\mathbf{x} \mid \mathbf{\mu}, \Lambda^{-1}), p(\mathbf{y} \mid \mathbf{x}) = \mathcal{N}(\mathbf{y} \mid A\mathbf{x} + \mathbf{b}, L^{-1})\\)로 주어질 때 \\(\mathbb{E}(\mathbf{x} \mid \mathbf{y}) = (\Lambda + A^T L A )^{-1} \left\\{A^T L (\mathbf{y} - \mathbf{b})+ \Lambda \mathbf{\mu} \right\\}, \mathrm{cov}(\mathbf{x} \mid \mathbf{y}) = (\Lambda + A^T L A )^{-1}\\) 이므로, 이를 이용하여 사후확률  \\(p(\mathbf{w} \mid \mathbf{t}) \\)를 다음과 같이 얻는다.

- \\(p(\mathbf{w} \mid \mathbf{t}) = \mathcal{N}( \mathbf{w} \mid \mathbf{m}_N, \mathbf{S}_N)\\)

  - \\(\mathbf{S} _ N = (\mathbf{S} _ {0}^{-1} + \beta \Phi^T \Phi )^{-1}, \mathbf{m} _ N = \mathbf{S}_N (\mathbf{S} _ {0}^{-1} \mathbf{m}_0 + \beta \Phi^T \mathbf{t} )\\)


\- 만약 사전확률을 \\(p(\mathbf{w} \mid \alpha) = \mathcal{N}(\mathbf{w} \mid , \alpha ^{-1} \mathbf{I})\\) 로 두면, 사후확률 \\(p(\mathbf{w} \mid \mathbf{t}) = \mathcal{N}( \mathbf{w} \mid \mathbf{m}_N, \mathbf{S}_N)\\) 의 \\(\mathbf{m} _ N = \beta \mathbf{S}_N  \Phi^T \mathbf{t}, \mathbf{S} _ N = (\alpha \mathbf{I} + \beta \Phi^T \Phi)^{-1}\\) 이 된다. 최대우도법으로 \\(p(\mathbf{w} \mid \mathbf{t}) \\)를 최대로 하는 \\(\mathbf{w}\\)를 구할 수 있다. 이때 \\(p(\mathbf{w} \mid \mathbf{t}) \\)에 로그를 취한 식이 최소제곱합에 L2 규제화항을 더한 식과 같은 꼴로 구해진다.


\- 새로운 입력 \\(\mathbf{x}\\)에 대한 예측값 \\(t\\)의 확률을 알고 싶을 때 베이즈 정리를 이용할 수 있다. 사전확률 \\(p(t \mid \mathbf{t}, \beta)\\), 우도 \\(p(\mathbf{w} \mid \mathbf{t}, \alpha, \beta)\\)라 할 때 사후확률 \\(p(t\mid \mathbf{t}, \alpha, \beta) = \int p(t \mid \mathbf{t}, \beta) p(\mathbf{w} \mid \mathbf{t}, \alpha, \beta) d \mathbf{w} = \mathcal{N} (t \mid \phi(\mathbf{x})^T\mathbf{m}_N, \beta^{-1} + \phi(\mathbf{x})^T (\phi(\mathbf{x})^T)^{-1} \phi(\mathbf{x}))\\) 이다.