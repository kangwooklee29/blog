---

title: SVD, PCA, 선형회귀

---


### 1. SVD

\- 모든 \\(m \times n\\) 행렬은 [\\(m \times m\\) 회전행렬(흔히 \\(U\\)라 한다)] \\(\times\\) [\\(m \times n\\) 대각행렬(흔히 \\(D\\)라 한다)] \\(\times\\) [\\(n \times n\\) 회전행렬(흔히 \\(V\\)라 한다)의 전치행렬] 꼴로 인수분해 할 수 있으며, 이러한 인수분해를 특이값 분해(singular value decomposition)이라 한다.

\- 행렬의 곱셉은 좌표계 변환으로 볼 수 있다고 했으므로, 특이값 분해를 통해 \\(m \times n\\) 행렬을 다음과 같은 과정을 거쳐 얻어낸 행렬로 볼 수 있다.

(1) n차원 직교좌표계 벡터(=항등행렬)를 회전행렬 V로 회전시킨다.

(2) 회전된 결과의 각 축을 대각행렬 D로 증폭시킨다. (m이 n보다 크다면, 이때 m-n개의 새로운 축이 추가된다.)

(3) 앞선 결과를 회전행렬 U로 회전시킨다.


\- 특이값 분해는 n개의 m-벡터가 주어졌을 때 **이들이 어떤 방향으로 강한 응집성을 보이는지 분석**하고 이들 벡터들과 유사한 방향으로 응집성을 가지면서 그 응집성이 더 강한 **근사 벡터들**을 구하는 데 응용할 수 있다. 어떤 \\(m \times n\\) 행렬이 있을 때, 이의 특이값 분해 U, D, V 행렬을 구한 후 각 행렬의 왼쪽에서부터 p개(단, n > p)의 열벡터를 골라(D 행렬의 대각선 성분이 **크기순으로 정렬돼서 구해지기 때문**에 왼쪽에서부터 순서대로 고르는 것이다) 이들로 새로운 U, D, V 행렬을 구해 곱하면 처음 행렬의 근사 행렬이 구해진다. 



### 2. PCA

\- 주성분 분석(principal component analysis)은, n개의 m차원 좌표 \\(\mathbf{x}_i\\)가 주어졌을 때 이 좌표들이 주로 분포하고 있는 방향벡터를 구하고 각 방향으로 얼만큼 분포하고 있는지를 그 n개의 좌표들로 만든 공분산 행렬을 이용해 분석하는 방법이다. 다음과 같은 방법으로 분석한다.

(1) \\(\mathbf{x}_i\\)의 중심 벡터를 \\(\mathbf{m}\\)이라 할 때, n개의 m차원 좌표 \\(\mathbf{x}_i\\)에 대하여 다음과 같이 정의되는 공분산 행렬 \\(C\\)를 구한다.

$$ C = 1 \over {n} \sum_{i=1}^n { (\mathbf{x}_i - \mathbf{m}) (\mathbf{x}_i - \mathbf{m})^T } $$


(2) 공분산 행렬 C는 [\\(n \times n\\) 회전행렬(흔히 \\(W\\)라 한다) ]\\(\times\\) [\\(n \times n\\) 대각행렬(흔히 \\(D\\)라 한다)] \\(\times\\) [\\(W^T\\) ] 꼴로 인수분해 할 수 있다. 이때 \\(W\\)와 \\(D\\)는 다음 의미를 갖는다.

- \\(W\\): 데이터들이 주로 분포하고 있는 방향벡터에 관한 정보를 담고 있는 회전행렬

- \\(D\\): 데이터들이 \\(W\\)의 각 벡터들의 방향으로 얼마나 응집되어 있는지를 담고 있는 대각행렬. 크기가 왼쪽 위 성분부터 오른쪽 아래 성분 순으로 정렬돼 있다.



### 3. 최소제곱법과 선형회귀


#### 1) column space

\- 행렬 \\(A\\)에 **가능한 모든 가중치 조합 열벡터** \\(\mathbf{x}\\)를 곱해 만든 행렬곱 \\(A\mathbf{x}\\)을 모은 집합을 열공간 \\(col(A)\\)라 한다. \\(\mathbf{x}\\)가 n-벡터라면 행렬곱 \\(A\mathbf{x}\\)의 결과값 또한 n-벡터이므로, 열공간 \\(col(A)\\)의 각 원소는 n-벡터이다.

\- 열공간 \\(col(A)\\)의 각 원소를 구하는 계산식은 linear equation이므로, 열공간 \\(col(A)\\)의 원소들을 좌표공간상에서 연결해 보면 linear한 모습을 갖는다. 

\- linear system \\(A\mathbf{x} = \mathbf{b}\\) 가 consistent 하다면 \\(\mathbf{b} \in col(A)\\) 이고, inconsistent 하다면 \\(\mathbf{x} \notin col(A)\\) 이다. 


#### 2) 최소제곱법

\- 앞서 말했듯 어떤 linear system \\(A\mathbf{x} = \mathbf{b}\\)가 inconsistent하다면, 이는 곧 우변의 열벡터 \\(\mathbf{b}\\)가 열공간 \\(col(A)\\)의 원소가 아님을 뜻한다. 그런데 \\(col(A)\\)**의 원소 중** \\(\left\| \mathbf{b} - \bar{\mathbf{b}} \right\|^2 \\)**의 값을 최소로 하는** \\(col(A)\\)**의 원소** \\(\bar{\mathbf{b}}\\)를 생각할 수 있다. 이러한 \bar{\mathbf{b}}의 값으로 **새로운 linear system** \\(A\bar{\mathbf{x}} = \bar{\mathbf{b}}\\)을 쓸 때, **그 해** \\(\bar{\mathbf{x}} \\)는 앞의 linear system \\(A\mathbf{x} = \mathbf{b}\\) 의 근사해라고 할 수 있다. 이러한 근사해를 최소제곱해(least squares solution)이라 하며, 최소제곱해를 구하는 것을 최소제곱법이라 한다.

\- 어떤 inconsistent한 linear system \\(A\mathbf{x} = \mathbf{b}\\)의 최소제곱해(=\\(A \bar{\mathbf{x}} = \bar{\mathbf{b}}\\)의 해)는 **이 linear system의 양변 각 항의 앞쪽에 \\(A\\)의 전치행렬 \\(A^T\\)를 곱한 새로운 linear system \\(A^T A \bar{\mathbf{x}} = A^T \mathbf{b}\\)의 해 \bar{\mathbf{x}}와 같음**이 알려져 있다. 


#### 3) 선형회귀

\- 선형회귀(linear regression)은 n개의 m차원 좌표가 주어졌을 때 이 좌표들을 '가장 잘 설명하는' linear equation을 구하는 방법이다. 주어진 좌표 \\(\mathbf{x}_i\\) 가 \\( \begin{bmatrix} x_{i, 1} & \cdots & x_{i, n} \end{bmatrix} \\)로 쓸 수 있고 구하고자 하는 linear equation이 \\(a_0 + a_1 x_1 + \cdots + a_{n-1} x_{n-1} = x_n\\) 이라면, 이 n개의 좌표들을 가장 잘 설명하는 linear equation의 계수 \\(a_i\\)는 다음 linear system의 최소제곱해이다.

$$ \begin{bmatrix} 1 & x_{1, 1} & \cdots & x_{1, n-1} \\
\vdots & \vdots & \vdots & \vdots \\
 1 & x_{n, 1} & \cdots & x_{n, n-1} \end{bmatrix} 
 \begin{bmatrix} a_0 \\
 \vdots \\
 a_{n-1} \end{bmatrix} =
 \begin{bmatrix} x_{1, n} \\
 \vdots \\
 x_{n, n} \end{bmatrix}
$$

