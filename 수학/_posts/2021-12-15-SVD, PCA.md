---

title: 고유값, SVD, PCA, 선형회귀

---


### 1. 고유값과 고유벡터

#### 1) 개요

\- 어떤 \\(n \times n\\) 행렬 \\(A\\)에 대해, \\(A \mathbf{v} = \lambda \mathbf{v}\\) 을 만족하는 **0 아닌 열벡터 \\(\mathbf{v}\\)와 상수** \\(\lambda\\)가 존재하는 경우가 있다. 이때의 열벡터 \\(\mathbf{v}\\)를 고유벡터, 상수 \\(\lambda\\)를 고유값이라 한다.

\- 기하학적으로 고유벡터와 고유값의 의미에 대해 생각해 보면, \\(n \times n\\) 행렬 \\(A\\)에 열벡터 \\(\mathbf{v}\\)를 곱한다는 것은 열벡터 \\(\mathbf{v}\\)를 행렬 \\(A\\)만큼 회전 변환을 수행한다는 의미이고, 그 결과값이 \\(\mathbf{v}\\)의 상수배라는 것은 **회전 변환에도 불구하고 그 열벡터만큼은 방향이 바뀌지 않았으되 다만 크기가 그 상수배만큼 바뀌었음**을 뜻한다. 

- 예를 들어 회전하는 구 내부 또는 표면상의 한 좌표가 구가 회전한 후 어떤 좌표값을 갖는지를 계산하는 문제를 생각할 수 있다. 이 회전을 뜻하는 변환 **행렬**의 **고유벡터**는 이 **구의 축**을 가리키는 벡터와 같고, **고유값**은 회전의 결과로 축 위의 어떤 점이 얼만큼 구의 중심에서 멀어지거나 가까워졌는지를 나타내는 지표와 같다고 말할 수 있다.

#### 2) 고유값과 고유벡터가 존재할 조건


$$

A \mathbf{v} = \lambda \mathbf{v}\quad\qquad\qquad  \\\\

(A-\lambda E)\mathbf{v} = \mathbf{0}\;\qquad \cdots(a)

$$

(1) 고유벡터가 존재할 조건

- 일단 정의에 의해, 고유값 \\(\lambda\\)가 존재해야 고유벡터 \\(\mathbf{v}\\)도 존재한다.

- 위 식 (a)를 보면, 행렬 \\((A-\lambda E)\\)가 역행렬이 존재한다면 \\(\mathbf{v}\\)의 값이 \\(\mathbf{0}\\)이 된다. 그러나 이는 정의와 맞지 않으므로, 결론적으로 **행렬 \\((A-\lambda E)\\)가 역행렬이 존재하지 않아야 행렬 \\(A\\)의 고유벡터가 존재한다.**


(2) 고유값이 존재할 조건

- 일단 정의에 의해, 고유벡터 \\(\mathbf{v}\\)가 존재해야 고유값 \\(\lambda\\)도 존재한다.

- 앞에서 고유벡터 \\(\mathbf{v}\\)가 존재하려면 행렬 \\((A-\lambda E)\\)가 역행렬이 존재하지 않아야 한다고 했는데 이는 곧 \\(det(A-\lambda E) = 0\\)이어야 함을 뜻한다. 이 식은 \\(\lambda\\)에 관한 방정식이므로, 결국 **이 방정식의 해가 존재해야 고유값 \\(\lambda\\)도 존재**한다.




### 2. SVD

#### 1) 선형독립(linearly independent)


\- 모든 \\(m \times n\\) 행렬은 [\\(m \times m\\) 회전행렬(흔히 \\(U\\)라 한다)] \\(\times\\) [\\(m \times n\\) 대각행렬(흔히 \\(D\\)라 한다)] \\(\times\\) [\\(n \times n\\) 회전행렬(흔히 \\(V\\)라 한다)의 전치행렬] 꼴로 인수분해 할 수 있으며, 이러한 인수분해를 특이값 분해(singular value decomposition)이라 한다.

\- 행렬의 곱셉은 좌표계 변환으로 볼 수 있다고 했으므로, 특이값 분해를 통해 \\(m \times n\\) 행렬을 다음과 같은 과정을 거쳐 얻어낸 행렬로 볼 수 있다.

(1) n차원 직교좌표계 벡터(=항등행렬)를 회전행렬 V로 회전시킨다.

(2) 회전된 결과의 각 축을 대각행렬 D로 증폭시킨다. (m이 n보다 크다면, 이때 m-n개의 새로운 축이 추가된다.)

(3) 앞선 결과를 회전행렬 U로 회전시킨다.


\- 특이값 분해는 n개의 m-벡터가 주어졌을 때 **이들이 어떤 방향으로 강한 응집성을 보이는지 분석**하고 이들 벡터들과 유사한 방향으로 응집성을 가지면서 그 응집성이 더 강한 **근사 벡터들**을 구하는 데 응용할 수 있다. 어떤 \\(m \times n\\) 행렬이 있을 때, 이의 특이값 분해 U, D, V 행렬을 구한 후 각 행렬의 왼쪽에서부터 p개(단, n > p)의 열벡터를 골라(D 행렬의 대각선 성분이 **크기순으로 정렬돼서 구해지기 때문**에 왼쪽에서부터 순서대로 고르는 것이다) 이들로 새로운 U, D, V 행렬을 구해 곱하면 처음 행렬의 근사 행렬이 구해진다. 



### 3. PCA

\- 주성분 분석(principal component analysis)은, n개의 m차원 좌표 \\(\mathbf{x}_i\\)가 주어졌을 때 이 좌표들이 주로 분포하고 있는 방향벡터를 구하고 각 방향으로 얼만큼 분포하고 있는지를 그 n개의 좌표들로 만든 공분산 행렬을 이용해 분석하는 방법이다. 다음과 같은 방법으로 분석한다.

(1) \\(\mathbf{x}_i\\)의 중심 벡터를 \\(\mathbf{m}\\)이라 할 때, n개의 m차원 좌표 \\(\mathbf{x}_i\\)에 대하여 다음과 같이 정의되는 공분산 행렬 \\(C\\)를 구한다.

$$ C = {1 \over {n} } \sum_{i=1}^n { (\mathbf{x}_i - \mathbf{m}) (\mathbf{x}_i - \mathbf{m})^T } $$


(2) 공분산 행렬 C는 [\\(n \times n\\) 회전행렬(흔히 \\(W\\)라 한다) ]\\(\times\\) [\\(n \times n\\) 대각행렬(흔히 \\(D\\)라 한다)] \\(\times\\) [\\(W^T\\) ] 꼴로 인수분해 할 수 있다. 이때 \\(W\\)와 \\(D\\)는 다음 의미를 갖는다.

- \\(W\\): 데이터들이 주로 분포하고 있는 방향벡터에 관한 정보를 담고 있는 회전행렬

- \\(D\\): 데이터들이 \\(W\\)의 각 벡터들의 방향으로 얼마나 응집되어 있는지를 담고 있는 대각행렬. 크기가 왼쪽 위 성분부터 오른쪽 아래 성분 순으로 정렬돼 있다.



### 4. 최소제곱법과 선형회귀


#### 1) column space

\- 행렬 \\(A\\)에 **가능한 모든 가중치 조합 열벡터** \\(\mathbf{x}\\)를 곱해 만든 행렬곱 \\(A\mathbf{x}\\)을 모은 집합을 열공간 \\(col(A)\\)라 한다. \\(\mathbf{x}\\)가 n-벡터라면 행렬곱 \\(A\mathbf{x}\\)의 결과값 또한 n-벡터이므로, 열공간 \\(col(A)\\)의 각 원소는 n-벡터이다.

\- 열공간 \\(col(A)\\)의 각 원소를 구하는 계산식은 linear equation이므로, 열공간 \\(col(A)\\)의 원소들을 좌표공간상에서 연결해 보면 linear한 모습을 갖는다. 

\- linear system \\(A\mathbf{x} = \mathbf{b}\\) 가 consistent 하다면 \\(\mathbf{b} \in col(A)\\) 이고, inconsistent 하다면 \\(\mathbf{x} \notin col(A)\\) 이다. 


#### 2) 최소제곱법

\- 앞서 말했듯 어떤 linear system \\(A\mathbf{x} = \mathbf{b}\\)가 inconsistent하다면, 이는 곧 우변의 열벡터 \\(\mathbf{b}\\)가 열공간 \\(col(A)\\)의 원소가 아님을 뜻한다. 그런데 \\(col(A)\\)**의 원소 중** \\(\left\| \mathbf{b} - \bar{\mathbf{b}} \right\|^2 \\)**의 값을 최소로 하는** \\(col(A)\\)**의 원소** \\(\bar{\mathbf{b}}\\)를 생각할 수 있다. 이러한 \\(\bar{\mathbf{b}}\\)의 값으로 **새로운 linear system** \\(A\bar{\mathbf{x}} = \bar{\mathbf{b}}\\)을 쓸 때, **그 해** \\(\bar{\mathbf{x}} \\)는 앞의 linear system \\(A\mathbf{x} = \mathbf{b}\\) 의 근사해라고 할 수 있다. 이러한 근사해를 최소제곱해(least squares solution)이라 하며, 최소제곱해를 구하는 것을 최소제곱법이라 한다.

\- 어떤 inconsistent한 linear system \\(A\mathbf{x} = \mathbf{b}\\)의 최소제곱해(=\\(A \bar{\mathbf{x}} = \bar{\mathbf{b}}\\)의 해)는 **이 linear system의 양변 각 항의 앞쪽에 \\(A\\)의 전치행렬 \\(A^T\\)를 곱한 새로운 linear system \\(A^T A \bar{\mathbf{x}} = A^T \mathbf{b}\\)의 해 \\(\bar{\mathbf{x}}\\)와 같음**이 알려져 있다. 


#### 3) 선형회귀

\- 선형회귀(linear regression)은 n개의 m차원 좌표가 주어졌을 때 이 좌표들을 '가장 잘 설명하는' linear equation을 구하는 방법이다. 주어진 좌표 \\(\mathbf{x}_i\\) 가 \\( \begin{bmatrix} x_{i, 1} & \cdots & x_{i, n} \end{bmatrix} \\)로 쓸 수 있고 구하고자 하는 linear equation이 \\(a_0 + a_1 x_1 + \cdots + a_{n-1} x_{n-1} = x_n\\) 이라면, 이 n개의 좌표들을 가장 잘 설명하는 linear equation의 계수 \\(a_i\\)는 다음 linear system의 최소제곱해이다.

$$ \begin{bmatrix} 1 & x_{1, 1} & \cdots & x_{1, n-1} \\
\vdots & \vdots & \vdots & \vdots \\
 1 & x_{n, 1} & \cdots & x_{n, n-1} \end{bmatrix} 
 \begin{bmatrix} a_0 \\
 \vdots \\
 a_{n-1} \end{bmatrix} =
 \begin{bmatrix} x_{1, n} \\
 \vdots \\
 x_{n, n} \end{bmatrix}
$$

