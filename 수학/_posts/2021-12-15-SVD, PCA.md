---

title: 고유값, SVD, PCA, 선형회귀

---


### 1. 고유값과 고유벡터

#### 1) 개요

\- 어떤 \\(n \times n\\) 행렬 \\(A\\)에 대해, \\(A \mathbf{v} = \lambda \mathbf{v}\\) 을 만족하는 **0 아닌 열벡터 \\(\mathbf{v}\\)와 상수** \\(\lambda\\)가 존재하는 경우가 있다. 이때의 열벡터 \\(\mathbf{v}\\)를 고유벡터, 상수 \\(\lambda\\)를 고유값이라 한다.

\- 기하학적으로 고유벡터와 고유값의 의미에 대해 생각해 보면, \\(n \times n\\) 행렬 \\(A\\)에 열벡터 \\(\mathbf{v}\\)를 곱한다는 것은 열벡터 \\(\mathbf{v}\\)를 행렬 \\(A\\)만큼 회전 변환을 수행한다는 의미이고, 그 결과값이 \\(\mathbf{v}\\)의 상수배라는 것은 **회전 변환에도 불구하고 그 열벡터만큼은 방향이 바뀌지 않았으되 다만 크기가 그 상수배만큼 바뀌었음**을 뜻한다. 

- 예를 들어 회전하는 구 내부 또는 표면상의 한 좌표가 구가 회전한 후 어떤 좌표값을 갖는지를 계산하는 문제를 생각할 수 있다. 이 회전을 뜻하는 변환 **행렬**의 **고유벡터**는 이 **구의 축**을 가리키는 벡터와 같고, **고유값**은 회전의 결과로 축 위의 어떤 점이 얼만큼 구의 중심에서 멀어지거나 가까워졌는지를 나타내는 지표와 같다고 말할 수 있다.


\- 행렬 \\(A\\)가 역행렬을 갖는다면 행렬 \\(A\\)는 고유값으로 0을 갖지 않는다.

#### 2) 고유값과 고유벡터가 존재할 조건


$$

A \mathbf{v} = \lambda \mathbf{v}\quad\qquad\qquad  

$$

$$

(A-\lambda E)\mathbf{v} = \mathbf{0}\;\qquad \cdots(a)

$$

(1) 고유벡터가 존재할 조건

- 일단 정의에 의해, 고유값 \\(\lambda\\)가 존재해야 고유벡터 \\(\mathbf{v}\\)도 존재한다.

- 위 식 (a)를 보면, 행렬 \\((A-\lambda E)\\)가 역행렬이 존재한다면 \\(\mathbf{v}\\)의 값이 \\(\mathbf{0}\\)이 된다. 그러나 이는 정의와 맞지 않으므로, 결론적으로 **행렬 \\((A-\lambda E)\\)가 역행렬이 존재하지 않아야 행렬 \\(A\\)의 고유벡터가 존재한다.**


(2) 고유값이 존재할 조건

- 일단 정의에 의해, 고유벡터 \\(\mathbf{v}\\)가 존재해야 고유값 \\(\lambda\\)도 존재한다.

- 앞에서 고유벡터 \\(\mathbf{v}\\)가 존재하려면 행렬 \\((A-\lambda E)\\)가 역행렬이 존재하지 않아야 한다고 했는데 이는 곧 \\(det(A-\lambda E) = 0\\)이어야 함을 뜻한다. 이 식은 \\(\lambda\\)에 관한 방정식이므로(이러한 방정식을 '특성방정식'이라 한다), 결국 **이 방정식의 해가 존재해야 고유값 \\(\lambda\\)도 존재**한다.

\* 참고로, 행렬 \\(A\\)가 역행렬을 갖지 않는 것과 고유값이 존재하는 것은 상관이 없다. 다만 \\(A\\)가 역행렬을 갖지 않는다면 고유값으로 0을 가짐이 알려져 있다.



#### 3) 정부호 행렬(definite matrix)

$$

\mathbf{x}^T A \mathbf{x} = 

\begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix} 

\begin{bmatrix} 

a_{1, 1} & \cdots & a_{1, n} \\
\vdots & \cdots & \vdots \\
a_{n, 1} & \cdots & a_{n, n}


\end{bmatrix} 

\begin{bmatrix} x_1 \\
 \vdots \\
  x_n \end{bmatrix} 

= a_{1, 1} x_1^2 + \cdots + a_{n, n} x_n^2


$$

\- 어떤 \\(n \times n\\) 정사각행렬 \\(A\\)가 있을 때, 0 아닌 모든 n-행벡터 \\(\mathbf{x}\\)에 대하여 \\(\mathbf{x}^T A \mathbf{x}\\)의 값이 0보다 크거나 작은 행렬을 정부호 행렬이라 한다. (0보다 크면 양의 정부호 행렬, 0보다 작으면 음의 정부호 행렬.)

\- 양의 정부호 행렬은 다음 성질을 갖는다.

- 고유값이 모두 양수다.

- 역행렬도 정부호 행렬이다.

- 역행렬이 존재한다.

- 임의의 행렬 \\(A\\)가 역행렬을 갖는다면 \\(A^T A\\)는 양의 정부호 대칭행렬이다. (역도 성립.)




### 2. 대칭행렬의 직교대각화

#### 1) 선형독립(linearly independent)

\- 어떤 \\(n \times n\\) 행렬 \\(A\\)가 있고 이를 구성하는 n차원 열벡터를 각각 \\(\mathbf{a}_1, \cdots, \mathbf{a}_n\\)라 할 때, 만약 \\( A \mathbf{x} = \mathbf{0}\\)**을 만족하는 열벡터 \\(\mathbf{x}\\)가 \\(\mathbf{0}\\)로 유일**하다면 이때 **열벡터 \\(\mathbf{a}_1, \cdots, \mathbf{a}_n\\)들끼리는 모두 선형독립**이라고 한다. 

\- 선형독립을 linear system의 관점에서 보면, 열벡터 \\(\mathbf{a}_1, \cdots, \mathbf{a}_n\\)이 모두 선형독립이라는 것은 **linear system \\( A \mathbf{x} = \mathbf{0}\\)의 해가 유일**함을 뜻한다. 이는 **행렬 \\(A\\)의 역행렬 \\(A^{-1}\\)이 존재한다는 것과 동치**이다.

\- 서로 선형독립 관계라면, 그 중 어느 한 벡터도 선형독립 관계인 다른 벡터들의 선형 결합으로 표현될 수 없다.

\- 직교와 선형독립: 서로 직교하는 관계인 벡터끼리는 서로 선형독립이기도 하다. (단, 선형독립 관계인 벡터는 반드시 서로 직교하는 것은 아니다.)


#### 2) 고유값 분해

\- \\(n \times n\\) 행렬이 고유값을 \\(n\\)개 이상 갖는다면 그 행렬을 그때의 고유벡터들로 만든 행렬과 각 성분이 고유값인 대각행렬의 곱셈 꼴로 표현할 수 있으며, 이를 고유값 분해(eigenvalue decomposition) 또는 대각화(diagonalization)라 한다.

\- 구체적으로, 어떤 \\(n \times n\\) 행렬 \\(A\\)의 고유값이 \\(\lambda_1\\)부터 \\(\lambda_n\\)까지 총 n개 있고 그때의 고유벡터가 \\(\mathbf{v}_1, \cdots, \mathbf{v}_n\\)이라 하면, 이 고유벡터들로 만든 행렬 \\(P = \begin{bmatrix} \mathbf{v}_1 & \cdots & \mathbf{v}_n \end{bmatrix}\\)에 대해 다음이 성립한다.

$$

AP = \begin{bmatrix} \lambda_1 \mathbf{v}_1 & \cdots & \lambda_n \mathbf{v}_n \end{bmatrix} = \begin{bmatrix} \mathbf{v}_1 & \cdots & \mathbf{v}_n \end{bmatrix} 

\begin{bmatrix} 
\lambda_1 & \cdots & 0 \\ 
\vdots & \ddots & \vdots \\ 
0 & \cdots & \lambda_n \\ 
\end{bmatrix}

$$

여기서 \\(\Lambda\\)를 다음과 같이 정의할 수 있다.

$$

\Lambda = 
\begin{bmatrix} 
\lambda_1 & \cdots & 0 \\ 
\vdots & \ddots & \vdots \\ 
0 & \cdots & \lambda_n \\ 
\end{bmatrix}

$$

이때 만약 \\(P^{-1}\\)이 존재한다면(=\\(P\\)가 선형독립이면) \\(A = P \Lambda P^{-1}\\)라는 식을 쓸 수 있다.

#### 3) 직교대각화

\- 어떤 행렬이 역행렬이 존재하는 **대칭행렬**이라면 이 행렬은 고유값 분해가 가능하며(\\(\because\\)역행렬이 존재하기 때문) 그 고유값 분해의 고유벡터 행렬은 **직교행렬**임이 알려져 있다. _(즉, 대칭행렬은 고유벡터끼리 서로 직교하며 또 서로 선형독립이기도 하다.)_ 이때 이러한 고유값 분해를 직교대각화라 한다.

\- 행렬 \\(A\\)가 \\(P \Lambda P^{-1}\\) 꼴로 직교대각화가 가능하다 할 때, \\(P^{-1} = P^T\\)이므로(\\(\because P\\)가 직교행렬이기 때문) \\(A = P \Lambda P^T\\)로 쓸 수도 있다. 

$$ 

A = P \Lambda P^{-1} = \begin{bmatrix} \mathbf{v}_1 & \cdots & \mathbf{v}_ n \end{bmatrix} 

\begin{bmatrix} 
\lambda _1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \lambda _n 
\end{bmatrix} 

\begin{bmatrix} \mathbf{v}_1 & \cdots & \mathbf{v}_ n \end{bmatrix}^T

$$

\- 직교대각화는 n개의 m-벡터가 주어졌을 때 **이들이 어떤 방향으로 강한 응집성을 보이는지 분석**하고 이들 벡터들과 유사한 방향으로 응집성을 가지면서 그 응집성이 더 강한 **근사 벡터들**을 구하는 데 응용할 수 있다. 다음과 같은 방법으로 할 수 있다.

(1) 주어진 벡터들로 \\(m \times n\\) 행렬을 만든 후, 이 행렬을 대각선 왼쪽 아래와 오른쪽 위 부분으로 나눈 후 각각에 대하여 정사각 대칭행렬을 만든 후 직교대각화를 한다. (각각의 고유벡터 행렬을 \\(P_1, P_2, \Lambda_1, \Lambda_2\\)라 하자.)

(2) \\(P_1, P_2, \Lambda_1, \Lambda_2\\)의 각 행렬의 왼쪽에서부터 p개(단, n > p)의 열벡터를 골라(단, \\(\Lambda_1, \Lambda_2\\) 행렬의 대각선 성분이 **크기순으로 정렬**돼있는 것으로 전제한다) 이들로 새로운 \\(P, \Lambda\\) 행렬을 구해 곱한 후 결과 행렬을 다시 대각선을 기준으로 나눈 후 이어붙이면 처음 행렬의 근사 행렬이 구해진다.





### 3. 특이값 분해


\- 모든 \\(m \times n\\) 행렬은 [\\(m \times m\\) 회전행렬(흔히 \\(U\\)라 한다)] \\(\times\\) [\\(m \times n\\) 대각행렬(흔히 \\(D\\)라 한다)] \\(\times\\) [\\(n \times n\\) 회전행렬(흔히 \\(V\\)라 한다)의 전치행렬] 꼴로 인수분해 할 수 있으며, 이러한 인수분해를 특이값 분해(singular value decomposition)이라 한다. 각 행렬은 직교대각화를 이용하여 구한다. (역행렬이 존재하는 어떤 행렬과 그 전치행렬의 곱은 대칭행렬이 된다는 성질을 이용한다. 참고로 이는 전치행렬의 자명한 성질인데 전치행렬은 \\((AB)^T = B^T A^T\\)가 성립하기 때문이다.)

\- 행렬의 곱셉은 좌표계 변환으로 볼 수 있다고 했으므로, 특이값 분해를 통해 \\(m \times n\\) 행렬을 다음과 같은 과정을 거쳐 얻어낸 행렬로 볼 수 있다.

(1) n차원 직교좌표계 벡터(=항등행렬)를 회전행렬 V로 회전시킨다.

(2) 회전된 결과의 각 축을 대각행렬 D로 증폭시킨다. (m이 n보다 크다면, 이때 m-n개의 새로운 축이 추가된다.)

(3) 앞선 결과를 회전행렬 U로 회전시킨다.

\- 특이값 분해를 하면 직교대각화보다 훨씬 더 간단한 방법으로 응집성을 분석할 수 있다. n개의 m-벡터로 \\(m \times n\\) 행렬을 만든 후, 이의 특이값 분해 U, D, V 행렬을 구해 각 행렬의 왼쪽에서부터 p개(단, n > p)의 열벡터를 골라(단, D 행렬의 대각선 성분이 **크기순으로 정렬**돼있는 것으로 전제한다) 이들로 새로운 U, D, V 행렬을 구해 곱하면 처음 행렬의 근사 행렬이 구해진다. 



### 4. PCA

\- 주성분 분석(principal component analysis)은, n개의 m차원 좌표 \\(\mathbf{x}_i\\)가 주어졌을 때 이 좌표들이 주로 분포하고 있는 방향벡터를 구하고 각 방향으로 얼만큼 분포하고 있는지를 그 n개의 좌표들로 만든 공분산 행렬을 이용해 분석하는 방법이다. 다음과 같은 방법으로 분석한다.

(1) \\(\mathbf{x}_i\\)의 중심 벡터를 \\(\mathbf{m}\\)이라 할 때, n개의 m차원 좌표 \\(\mathbf{x}_i\\)에 대하여 다음과 같이 정의되는 공분산 행렬 \\(C\\)를 구한다.

$$ C = {1 \over {n} } \sum_{i=1}^n { (\mathbf{x}_i - \mathbf{m}) (\mathbf{x}_i - \mathbf{m})^T } $$


(2) 공분산 행렬 C는 [\\(n \times n\\) 회전행렬(흔히 \\(W\\)라 한다) ]\\(\times\\) [\\(n \times n\\) 대각행렬(흔히 \\(D\\)라 한다)] \\(\times\\) [\\(W^T\\) ] 꼴로 인수분해 할 수 있다. 이때 \\(W\\)와 \\(D\\)는 다음 의미를 갖는다.

- \\(W\\): 데이터들이 주로 분포하고 있는 방향벡터에 관한 정보를 담고 있는 회전행렬

- \\(D\\): 데이터들이 \\(W\\)의 각 벡터들의 방향으로 얼마나 응집되어 있는지를 담고 있는 대각행렬. 크기가 왼쪽 위 성분부터 오른쪽 아래 성분 순으로 정렬돼 있는 것으로 전제한다.



### 5. 최소제곱법과 선형회귀


#### 1) column space

\- 행렬 \\(A\\)에 **가능한 모든 가중치 조합 열벡터** \\(\mathbf{x}\\)를 곱해 만든 행렬곱 \\(A\mathbf{x}\\)을 모은 집합을 열공간 \\(col(A)\\)라 한다. \\(\mathbf{x}\\)가 n-벡터라면 행렬곱 \\(A\mathbf{x}\\)의 결과값 또한 n-벡터이므로, 열공간 \\(col(A)\\)의 각 원소는 n-벡터이다.

\- 열공간 \\(col(A)\\)의 각 원소를 구하는 계산식은 linear equation이므로, 열공간 \\(col(A)\\)의 원소들을 좌표공간상에서 연결해 보면 linear한 모습을 갖는다. 

\- linear system \\(A\mathbf{x} = \mathbf{b}\\) 가 consistent 하다면 \\(\mathbf{b} \in col(A)\\) 이고, inconsistent 하다면 \\(\mathbf{x} \notin col(A)\\) 이다. 


#### 2) 최소제곱법

\- 앞서 말했듯 어떤 linear system \\(A\mathbf{x} = \mathbf{b}\\)가 inconsistent하다면, 이는 곧 우변의 열벡터 \\(\mathbf{b}\\)가 열공간 \\(col(A)\\)의 원소가 아님을 뜻한다. 그런데 \\(col(A)\\)**의 원소 중** \\(\left\| \mathbf{b} - \bar{\mathbf{b}} \right\|^2 \\)**의 값을 최소로 하는** \\(col(A)\\)**의 원소** \\(\bar{\mathbf{b}}\\)를 생각할 수 있다. 이러한 \\(\bar{\mathbf{b}}\\)의 값으로 **새로운 linear system** \\(A\bar{\mathbf{x}} = \bar{\mathbf{b}}\\)을 쓸 때, **그 해** \\(\bar{\mathbf{x}} \\)는 앞의 linear system \\(A\mathbf{x} = \mathbf{b}\\) 의 근사해라고 할 수 있다. 이러한 근사해를 최소제곱해(least squares solution)이라 하며, 최소제곱해를 구하는 것을 최소제곱법이라 한다.

\- 어떤 inconsistent한 linear system \\(A\mathbf{x} = \mathbf{b}\\)의 최소제곱해(=\\(A \bar{\mathbf{x}} = \bar{\mathbf{b}}\\)의 해)는 **이 linear system의 양변 각 항의 앞쪽에 \\(A\\)의 전치행렬 \\(A^T\\)를 곱한 새로운 linear system \\(A^T A \bar{\mathbf{x}} = A^T \mathbf{b}\\)의 해 \\(\bar{\mathbf{x}}\\)와 같음**이 알려져 있다. 


#### 3) 선형회귀

\- 선형회귀(linear regression)은 n개의 m차원 좌표가 주어졌을 때 이 좌표들을 '가장 잘 설명하는' linear equation을 구하는 방법이다. 주어진 좌표 \\(\mathbf{x}_i\\) 가 \\( \begin{bmatrix} x_{i, 1} & \cdots & x_{i, n} \end{bmatrix} \\)로 쓸 수 있고 구하고자 하는 linear equation이 \\(a_0 + a_1 x_1 + \cdots + a_{n-1} x_{n-1} = x_n\\) 이라면, 이 n개의 좌표들을 가장 잘 설명하는 linear equation의 계수 \\(a_i\\)는 다음 linear system의 최소제곱해이다.

$$ \begin{bmatrix} 1 & x_{1, 1} & \cdots & x_{1, n-1} \\
\vdots & \vdots & \vdots & \vdots \\
 1 & x_{n, 1} & \cdots & x_{n, n-1} \end{bmatrix} 
 \begin{bmatrix} a_0 \\
 \vdots \\
 a_{n-1} \end{bmatrix} =
 \begin{bmatrix} x_{1, n} \\
 \vdots \\
 x_{n, n} \end{bmatrix}
$$

