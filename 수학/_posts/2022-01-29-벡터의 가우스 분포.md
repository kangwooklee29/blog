### 1. 개요

확률변수인 \\(D\\)-벡터 \\(\mathbf{x}\\)가 \\(D\\)-벡터 \\(\mathbf{\mu}\\)를 평균으로 하고 \\(D \times D\\) 행렬 \\(\Sigma\\)를 공분산 행렬으로 하는 가우스 분포를 따른다 할 때, 이를 다음과 같이 쓴다.

$$

\mathcal{N} ( \mathbf{x} \mid \mathbf{\mu}, \Sigma) = {1 \over {(2\pi)^{D/2}} } { 1 \over {\| \Sigma \|^{1/2} } } \mathrm{exp} \left\{ - {1 \over 2} (\mathbf{x} - \mathbf{\mu})^T \Sigma^{-1} (\mathbf{x} - \mathbf{\mu}) \right\}

$$

여기서 \\(\Sigma\\)는 원래는 대칭행렬로 주어지지 않지만, exp 함수의 지수부에는 이차형식 꼴로 쓰여 있으므로 여기서의 \\(\Sigma\\)는 대칭행렬이라고 간주할 수 있다. 그런데 대칭행렬 \\(\Sigma\\)는 다음과 같은 고유값 분해가 가능하다.

$$

\begin{matrix}
\Sigma &=& U^T \Lambda U &=& \begin{bmatrix} \mathbf{u}_1 & \cdots & \mathbf{u}_ D \end{bmatrix}^T 

\begin{bmatrix} 
\lambda _1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \lambda _D
\end{bmatrix} 
\begin{bmatrix} \mathbf{u}_1 & \cdots & \mathbf{u}_ D \end{bmatrix} \\

&=& \displaystyle \sum_{i=1}^D \lambda_i \mathbf{u}_i \mathbf{u}_i ^T 

\end{matrix}

$$

여기서, \\(\Sigma^{-1} = \displaystyle \sum_{i=1}^D {1 \over \lambda_i} \mathbf{u}_i \mathbf{u}_i ^T\\)을 얻는다. 이를 이용하여 다음과 같이 쓸 수 있다.

$$

(\mathbf{x} - \mathbf{\mu})^T \Sigma^{-1} (\mathbf{x} - \mathbf{\mu})  = \sum_{i=1}^D {1 \over \lambda_i} 

\left\{ \mathbf{u}_i^T(\mathbf{x} - \mathbf{\mu}) \right\}^2



$$




### 2. 조건부 가우스 분포

#### 1) 확률변수 벡터의 분할

\- 확률변수인 \\(D\\)-벡터 \\(\mathbf{x}\\)가 가우스 분포 \\(\mathcal{N} ( \mathbf{x} \mid \mathbf{\mu}, \Sigma)\\)를 따를 때, 다음과 같이 분할이 가능하다고 하자.

(1) \\(\mathbf{x} = \begin{bmatrix} \mathbf{x}_a \\\ \mathbf{x}_b \end{bmatrix}\\) (단, \\(\mathbf{x}_a\\)는 \\(M\\)-벡터, \\(\mathbf{x}_b\\)는 \\((D-M)\\)-벡터)

(2) \\(\mathbf{\mu} = \begin{bmatrix} \mathbf{\mu}_a \\\ \mathbf{\mu}_b \end{bmatrix}\\) (단, \\(\mathbf{\mu}_a\\)는 \\(M\\)-벡터, \\(\mathbf{\mu}_b\\)는 \\((D-M)\\)-벡터)

(3) \\(\Sigma = \begin{bmatrix} \Sigma_ {a, a} & \Sigma_{a, b}  \\\ \Sigma _{b, a} &  \Sigma _{b, b}  \end{bmatrix}\\) (단, \\( \Sigma _{a, a} \\)는 \\(M \times M\\) 행렬, \\( \Sigma _{a, b} \\)는 \\(M \times (D-M)\\) 행렬, \\( \Sigma _{b, a} \\)는 \\((D-M) \times M\\) 행렬, \\( \Sigma _{b, b} \\)는 \\((D-M) \times (D-M)\\) 행렬)


\- 한편 앞으로 전개에서 \\(\Sigma^{-1}\\)의 각 성분이 자주 나오므로 다음과 같은 정의를 사용한다.

$$

\Sigma^{-1} = \Lambda = \begin{bmatrix} \Lambda_ {a, a} &  \Lambda_{a, b}  \\ \Lambda _{b, a} &  \Lambda _{b, b}  \end{bmatrix}


$$

 (단, \\( \Lambda _{a, a} \\)는 \\(M \times M\\) 행렬, \\( \Lambda _{a, b} \\)는 \\(M \times (D-M)\\) 행렬, \\( \Lambda _{b, a} \\)는 \\((D-M) \times M\\) 행렬, \\( \Lambda _{b, b} \\)는 \\((D-M) \times (D-M)\\) 행렬)



#### 2) 조건부확률 \\(p(\mathbf{x}_a \mid \mathbf{x}_b)\\)

\- 결합확률분포 \\(p(\mathbf{x}_a, \mathbf{x}_b)\\)에서 \\(\mathbf{x}_b\\)를 고정된 관찰값이라 가정하면 이때의 \\(\mathbf{x}_a\\)의 확률분포를 알 수 있는데, 이는 \\(p(\mathbf{x}_a \mid \mathbf{x}_b)\\)와 같다.

\- 그런데 \\(p(\mathbf{x}_a, \mathbf{x}_b)\\)는 위에서 조사했던 \\(\mathbf{x}\\)의 확률분포와 동일하다. 따라서 \\(\mathbf{x}\\)를 분할한 벡터로 보고 전개하면 완전제곱꼴-계수비교를 활용하여 \\(p(\mathbf{x}_a \mid \mathbf{x}_b)\\)의 평균 \\(\mathbf{\mu} _{a \mid b} = \mathbf{\mu}_a - \Lambda _{a, a}^{-1}\Lambda _{a, b}^{-1}(\mathbf{x}_b - \mathbf{\mu}_b)\\)와 분산 \\(\Sigma _{a \mid b} - \Lambda _{a, a}^{-1}\\)를 얻는다.



### 3. 주변 가우스 분포

\- 예를 들어 결합확률분포 \\(p(\mathbf{x}_a, \mathbf{x}_b)\\)가 있을 때, \\(p(\mathbf{x}_a) = \displaystyle \int p(\mathbf{x}_a, \mathbf{x}_b) d \mathbf{x}_b\\)나 \\(p(\mathbf{x}_b) = \displaystyle \int p(\mathbf{x}_a, \mathbf{x}_b) d \mathbf{x}_a\\) 처럼 어느 한 쪽의 확률변수를 없앤 확률분포를 주변확률분포라 한다. 

\- 주변확률분포의 적분을 계산하여 \\(p(\mathbf{x}_a, \mathbf{x}_b)\\)의 \\(\mathbf{x}_a, \mathbf{x}_b\\)에 대하여 \\(\mathcal{N} ( \mathbf{x}_a \mid \mathbf{\mu}_a, \Sigma _{a, a})\\), \\(\mathcal{N} ( \mathbf{x}_b \mid \mathbf{\mu}_b, \Sigma _{b, b})\\) 임을 알 수 있다.



### 4. 벡터 가우스 분포의 베이즈 정리

\- 확률분포 \\(p(\mathbf{x}) = \mathcal{N}(\mathbf{x} \mid \mathbf{\mu}, \Lambda^{-1}), p(\mathbf{y} \mid \mathbf{x}) = \mathcal{N}(\mathbf{y} \mid A\mathbf{x} + \mathbf{b}, L^{-1})\\)로 주어질 때, \\(p(\mathbf{y}), p(\mathbf{x} \mid \mathbf{y})\\)의 평균과 공분산을 구하는 문제는 결합확률분포 \\(p(\mathbf{x}, \mathbf{y})\\)를 구하여 풀 수 있다.

- \\(\mathbb{E}(\mathbf{y}) = A \mathbf{\mu} + \mathbf{b}, \mathrm{cov}(\mathbf{y}) = L^{-1} + A \Lambda^{-1} A^T\\)

- \\(\mathbb{E}(\mathbf{x} \mid \mathbf{y}) = (\Lambda + A^T L A )^{-1} \left\\{A^T L (\mathbf{y} - \mathbf{b})+ \Lambda \mathbf{\mu} \right\\}, \mathrm{cov}(\mathbf{x} \mid \mathbf{y}) = (\Lambda + A^T L A )^{-1}\\)
 



### 5. 벡터 가우스 분포와 최대우도법, 베이즈 정리

#### 1) 최대우도법으로 벡터 가우스 분포의 평균, 공분산 구하기

\- 가우스 분포를 따르는 어떤 데이터셋 \\(\mathbf{X} = \begin{bmatrix} \mathbf{x}_1 \\\ \vdots \\\ \mathbf{x}_n \end{bmatrix}\\)이 주어질 때, 이 데이터셋에 대한 우도함수를 최대화하는 평균과 공분산을 구할 수 있다. 

- \\(\mathbf{\mu}_{ML} =\displaystyle {1 \over N }  \sum _{i=1}^N \mathbf{x}_i = \bar{\mathbf{x}}\\)

- \\(\Sigma_{ML} = \displaystyle{1 \over N }  \sum _{i=1}^N (\mathbf{x}_i - \mathbf{\mu}) (\mathbf{x}_i - \mathbf{\mu})^T\\)


#### 2) 베이즈 정리로 데이터셋의 평균이 갖는 확률분포 구하기

\- 사전확률 \\(p(\mathbf{\mu}\\)와 우도 \\(p(\mathbf{X} \mid \mathbf{\mu}\\)가 주어진다면 사후확률 \\(p(\mathbf{\mu} \mid \mathbf{X})\\)를 베이즈 정리로 구할 수 있다.

- \\(p(\mathbf{\mu} = \mathcal{N}(\mathbf{\mu} \mid \mu_0, \sigma_{0}^2)\\) 이고 \\(p(\mathbf{X} \mid \mathbf{\mu} = \displaystyle \prod _ {n=1}^N p(\mathbf{x} _ n \mid \mathbf{\mu}) \\) 라 하면, \\(p(\mathbf{\mu} \mid \mathbf{X}) = \mathcal{N}(\mathbf{\mu} \mid \mu_N, \sigma _ {N}^2)\\) 이다.

  - \\(\mathbf{\mu} _ N = { {\sigma^2}  \over {N \sigma _ {0}^2 + \sigma^2 } } (\mu_0 + \sum _ {n=1}^N \mathbf{x} _ n )\\)

  - \\(\displaystyle {1 \over {\sigma_{N}^2} }= {1 \over {\sigma_{0}^2} } + {N \over {\sigma^2} }\\)
