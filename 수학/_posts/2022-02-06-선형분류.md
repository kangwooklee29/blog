
### 1. 개요

\- 입력벡터(\\(\mathbf{x}\\))를 수개의 클래스 중 하나의 클래스로 할당하는 문제(예를 들면, 클래스가 \\(C_1, \cdots, C_K\\)로 주어질 때 \\(\mathbf{x}\\)를 이 중 하나로 분류하는 문제)를 분류문제라 한다. 분류문제를 풀기 위한 결정이론으로서 \\(k=1, \cdots, K\\)에 대해 모든 사후확률 \\(p(C_k \mid \mathbf{x})\\)을 구해볼 수 있다. (이를 구하기 위하여, 이 값을 직접 모델링해 구하는 discriminative model과 사전확률과 우도를 모델링한 후 베이즈 정리로 구하는 generative model이 있다.) 또는, 분류문제를 풀기 위해 주어진 벡터를 입력으로 하여 수개의 클래스 중 하나의 클래스를 함수값으로 내는 discriminant function을 구해볼 수 있다. (이 경우 확률을 계산하지는 않는다.)


### 2. linear한 discriminant function

#### 1) 출력으로 0 또는 1만 내는 linear discriminant function

\- \\(y(\mathbf{w}) = \mathbf{w}^T \mathbf{x} + w_0\\)라는 선형함수가 있을 때, 이 함수값이 0 이상이면 그 \\(\mathbf{x}\\)를 \\(C_1\\)으로 분류하고 0 미만이면 \\(C_2\\)로 분류하는 함수를 생각할 수 있다. 

\- 이 판별함수의 decision boundary는 \\(y(\mathbf{x})=0\\)이다. 

\- decision boundary를 가리키는 벡터 \\(\mathbf{w}\\)가 있고 임의의 벡터 \\(\mathbf{x}\\)가 있다 할 때, \\(\mathbf{x}\\)의 decision boundary에 대한 사영 벡터를 \\(\mathbf{x}_{\perp} = \mathbf{x}- {y(\mathbf{x}) \over {\|\mathbf{w}\|}^2} \mathbf{w}\\)라 하자. 

- \\(\mathbf{x} - \mathbf{x}_{\perp} = {y(\mathbf{x}) \over {\|\mathbf{w}\|}^2} \mathbf{w}\\) 이므로, \\(y(\mathbf{x})\\)가 0보다 크면 decision boundary를 기준으로 \\(\mathbf{x}\\)가 \\(\mathbf{w}\\)와 같은 방향, 0보다 작으면 \\(\mathbf{w}\\)와 반대 방향에 있다는 것을 알 수 있다.

#### 2) 클래스가 \\(K\\)개인 판별함수

\- 클래스가 2개가 아니라 \\(K\\)개라면, 선형함수 \\(y_k(\mathbf{w}) = \mathbf{w}_{k}^T \mathbf{x} + w _ {k,0}\\) (\\(k = 1, \cdots, K\\))에 대해 판별함수는 '\\(y_k(\mathbf{w})\\)가 \\(y_j(\mathbf{w})\\)(\\(j=1, \cdots, K\\)이면서 \\(j \ne k\\))보다 크면 \\(\mathbf{x}\\)를 \\(C_k\\)로 판별하는 함수'라고 쓸 수 있다.

- 이를 \\(\tilde{\mathbf{x} } = \begin{bmatrix} x_0 & \mathbf{x}^T \end{bmatrix}^T \\)와 \\(k\\)번째 열이 \\(\begin{bmatrix} w_{k, 0} & \mathbf{w}_{k}^T \end{bmatrix}^T\\)인 행렬 \\(\tilde{W}\\)를 사용하여, \\(y(\mathbf{x}) = \tilde{W}^T \tilde{\mathbf{x} }\\) 라고 쓸 수도 있다.


### 3. 선형분류의 제곱합 에러함수와 \\(\tilde{W}\\)의 최적값

\- 학습데이터 \\(\left\\{\mathbf{x}_n, \mathbf{t}_n\right\\}\\) , \\(n\\)번째 행이 \\(\mathbf{t} _ {n}^T\\)인 행렬 \\(T\\), \\(n\\)번째 행이 \\(\tilde{\mathbf{x} } _ {n}^T\\)인 행렬 \\(\tilde{X}\\) (\\(n=1, \cdots, N\\)) 가 주어질 때 제곱합 에러함수는 \\(E_D(\tilde{W}) = {1 \over 2} \mathrm{tr} \left\\{(\tilde{X} \tilde{W} - T)^T(\tilde{X} \tilde{W} - T)\right\\}\\) 로 쓸 수 있다.

\- 최대우도법으로 \\(E_D(\tilde{W})\\)를 최소로 하는 \\(\tilde{W}\\)를 구하면 \\(\tilde{W} = (\tilde{X}^T\tilde{X})^{-1}\tilde{X}^T T\\) 이다.



### 4. 퍼셉트론

\- 퍼셉트론은 \\(f(a) = \begin{cases} 1, & a \ge 0 \\\ -1, & a < 0 \end{cases} \\)와 같은 계단형 함수를 사용한다. 따라서 분류함수 \\(y(\mathbf{x}) = f( \mathbf{w}^T \phi(\mathbf{x}))\\) 또한 퍼셉트론이다.

\- \\(y(\mathbf{x})\\)가 잘못 분류한 데이터 집합 \\(\mathcal{M}\\)에 대해 에러함수 \\(E_P(\mathbf{w}) = - \displaystyle \sum_{n \in \mathcal{M} } \mathbf{w}^T \phi_n \t_n \\) 이다.