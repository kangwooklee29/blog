
### * 분류문제에서 최대우도법으로 가중치 벡터 구하기

#### 1) 훈련 데이터셋을 이용한 우도함수 만들기

\- 어떤 실수 \\(x_i\\)를 입력받으면 그에 대한 결과로 \\(t_i\\)(\\(i = 1, \cdots, n\\))가 나와야 하는 훈련 데이터셋이 있다 하자. **이 훈련 데이터셋이 정규분포에서 추출된 표본이라 가정**하고 **최대우도법**으로 예측함수의 가중치 벡터 \\(\mathbf{w}\\)를 추측할 수 있다. 

\- 주어진 어떤 실수 \\(x\\)와 가중치 벡터 \\(\mathbf{w}\\)에 대한 예측함수 \\(y(x, \mathbf{w})\\)의 값을 평균으로 하고 또 주어진 임의의 실수 \\(\beta^{-1}\\)을 분산으로 하는 정규분포 \\(\mathcal{N}(t \mid y(x, \mathbf{w}), \beta^{-1}) \\)에서 임의의 표본을 \\(n\\)개 추출했을 때, 이 표본들이 추출될 확률 \\(	\prod_{n=1}^N \mathcal{N}(t_n \mid y(x_n, \mathbf{w}), \beta^{-1})\\)은 \\(\mathbf{w}, \beta^{-1}\\)의 우도함수 \\(p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta)\\)가 된다. (\\(\mathbf{t}\\)와 \\(\mathbf{X}\\)는 각각 \\(x_i\\), \\(t_i\\)((\\(i = 1, \cdots, n\\))를 각 성분으로 하는 열벡터.) 이 우도함수에 마이너스 로그를 취한 식은 분류문제의 손실함수의 최소제곱합 식과 매우 유사한 꼴을 갖는다.

#### 2) 베이즈 정리를 이용해 사전확률식을 곱한 우도함수 만들기

\- 예측함수의 가중치 벡터 \\(\mathbf{w}\\)의 각 성분이 평균이 0이고 분산이 \\(\alpha^{-1} \mathbf{I}\\)인 정규분포를 따른다고 가정하자. 이때 우도함수 \\(p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta)\\)는 베이즈 정리의 사전확률, 우도함수 \\(p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta)\\)는 베이즈 정리의 우도로 볼 수 있어 이 둘의 곱은 베이즈 정리의 사후확률 \\(p(\mathbf{w} \mid \mathbf{X}, \mathbf{w}, \alpha, \beta)\\)에 비례한다고 볼 수 있다. 따라서 이를 통해 사후확률을 최대화하는 \\(\mathbf{w}\\)의 값을 구하는 식을 얻을 수 있다. 그런데 이 사후확률 식에 마이너스 로그를 취한 식을 구해보면 분류문제의 최소제곱합 손실함수에 규제화 항을 더한 식과 유사한 식을 얻을 수 있다.


#### 3) 베이지안 곡선 근사

\-  \\(p(t \mid \mathbf{X}, \mathbf{w}, x) = \int  p(t \mid x, \mathbf{w})  p(\mathbf{w} \mid \mathbf{X}, \mathbf{t}) d \mathbf{w}\\)