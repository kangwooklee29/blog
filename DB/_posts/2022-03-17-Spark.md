#### 0. 설치

```python
!pip install pyspark py4j
```



#### 1. SparkSession 객체 만들기

\- SparkSession은 Spark를 사용하고자 할 경우 가장 먼저 선언해야 하는 객체이다. 이 객체가 지원하는 여러 변수, 메서드를 사용하여 Spark를 사용할 수 있다.

```python
from pyspark.sql import SparkSession

spark1 = SparkSession.builder.appName("app1").getOrCreate()
```

#### 2. RDD 다루기

(1) Python의 리스트 같은 변수형을 RDD형으로 변환하기

```python
list1 = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]
rdd1 = spark1.sparkContext.parallelize(list1)
```

\- parallelize() 메서드를 호출한다 하여 곧바로 rdd1 변수에 RDD형으로 변환된 값이 저장되는 게 아님을 유의해야 한다. rdd1의 메서드를 호출하는 문장이 나타날 때 그 메서드를 호출하면서 그때부터 비로소 rdd1 변수에 RDD형으로 변환된 값이 저장된다. (이를 lazy execution이라 한다.)

(2) 서버에 저장돼 있는 RDD형 변수의 내용 가져오기

```python
print(rdd1.collect())
```

\- 실제로 서버에 저장돼 있는 RDD형 데이터의 크기는 굉장히 큰 경우가 많으므로, 이와 같은 방식으로 그 내용을 가져오게 하면 굉장히 많은 자원 소모가 있게 된다. 따라서 RDD형 데이터의 크기가 아주 작은 경우가 아닐 때에는 이 코드를 사용할 일은 없다.


#### 3. Dataframe 다루기

(1) Dataframe 생성

- RDD형 데이터를 Dataframe형으로 변환하기

```python
df1 = rdd1.toDF()
```

- DB에 접속해 쿼리를 보내 그 쿼리에 해당하는 데이터를 가져와 Dataframe형으로 저장하기

```python
df2 = spark.read.format("jdbc").option("url", "postgresql://host1:1234/db1") \
        .option("dbtable", "table1").option("user", "guest1").option("password", "guest123") \
        .option("driver", "org.postgresql.Driver").load()
```

